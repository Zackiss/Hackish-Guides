# 多层感知机

### 网络结构

- 图运算构成
    
    <aside>
    💡 多层感知机的本质为一个全连接的图结构
    
    </aside>
    
    - 对以下字母表示的解释
        
        我们以两邻近层（第零层 $L_0$，第一层 $L_1$）之间的两节点（Neuron）为例：
        
        1. 两节点表示为 $a_6^{\tiny(0)},a_2^{\tiny(1)}$，每个节点均承载了一个 $[0,1]$ 之间的数字
        2. 两节点之间的边称为权重 $w_{6,2}$，作为整层节点计算加权和时所用权值
        3. 在加权和结尾减去的值 $b_0$ 为修正加权和的一个偏差值（Bias)
    
    对于第零层 $L_0$ 与第一层 $L_1$ 之间的运算，我们有节点 $a_{n}^{(1)} \in L_1$，其值为：
    
    $$
    a_{n}^{\tiny(1)}=\sigma(w_{0,0}\cdot a_0^{\tiny(0)}+w_{0,1}\cdot a_1^{\tiny(0)}+\cdots+w_{0,n}\cdot a_n^{\tiny(0)}-b_0)
    $$
    
    即每一个下层节点的值都是上层所有节点的加权和。
    
    其中，$\sigma$ 称为逻辑斯蒂函数，起到压缩加权和的作用：
    
    $$
    \sigma(x)=\frac{1}{1+e^{-x}}
    $$
    
    最后，我们有如下过程：
    
    1. 计算上一层所有节点值的加权和。
    2. 在加权和中减去偏差值，防止过拟合。
    3. 利用逻辑斯蒂函数将结果压缩到 $[0, 1]$ 区间内。
    
    我们可以将如上过程重构为一个矩阵运算：
    
    $$
    \left(\begin{array}{cccc}a_0^{\tiny(1)} \\a_1^{\tiny(1)}\\\vdots\\a_n^{\small\tiny(1)}\end{array}\right)=\left(\begin{array}{cccc}w_{0,0} & w_{0,1} & \cdots & w_{0,n} \\ w_{1,0} & w_{1,1} & & w_{1,n} \\ \vdots & \vdots & \ddots & \vdots \\ w_{k,0} & w_{k,1} & \cdots & w_{k,n}\end{array}\right)\cdot \left(\begin{array}{cccc}a_0^{\tiny(0)} \\a_1^{\tiny(0)}\\\vdots\\a_n^{\small\tiny(0)}\end{array}\right)+\left(\begin{array}{cccc}b_0 \\b_1\\\vdots\\b_n\end{array}\right)
    $$
    
    我们将如上矩阵运算简写为：
    
    $$
    a^{\tiny(1)}=\sigma(W\cdot a^{\tiny(0)}+b)
    $$
    
    由此，我们成功构建出了多层感知机图结构的运算过程，如果我们有合适的权重值，我们可以利用多层感知机来拟合任意一种函数（所描述的的输入输出对应关系）。
    
- 反馈过程
    
    <aside>
    💡 反馈过程中，我们一般使用梯度下降算法来更新权重
    
    </aside>
    
    当我们神经网络预测时，预测值与真实值之间存在差距，我们将预测值与真实值之间的差异程度称为损失函数。因此，损失函数的值越大，模型预测能力越差，越小则说明预测效果越好。
    
    反馈过程，即学习过程的本质是通过调整神经网络内参数，找到损失函数的最小值。
    
    我们构造损失函数，基本要求是该函数连续可导。
    
    因此我们有 $N$ 个内部权重，我们要找到一列权重偏置，根据此调整内部权重，使得损失函数达到局部最小值。
    
    在这里，我们选择这样一个损失函数，为最小二乘法：
    
    $$
    \text{loss}=\sum_i (O_i-E_i)^2
    $$
    
    其中，$O_i$ 为输出层中，第 $i$ 个输出节点的值，$E_i$ 为该值的预期值。
    
    对于每一次拟合过程，我们都有如上一个损失函数。我们进行 $n$ 次不同输入与不同预期输出的实验，取得这些实验的损失函数结果的平均值，这就是整个模型的损失值（total cost）。
    
    优化问题的本质是通过改变权重，使每次实验的损失值最小，亦即如下表示：
    
    $$
    \argmin_{w\in W,b\in B}\sum_i (O_i-E_i)^2
    $$
    
    其中，$W$ 为神经网络内的所有权重的集合，$B$ 为所有偏置的集合。
    
    一般而言，我们的常规思路是将整个神经网络作为一个连续可微函数，将所有权重作为数个维度的自变量，接着利用拉格朗日乘子法，或单纯求解函数梯度，使其为零，进而解出每一个参数的值。但由于自变量过多，维度过高，导致这样的方程几乎是一个不可解的问题。
    
    因此我们使用如下办法，分步骤地，递归地寻找到局部较优的一列权重偏置：
    
    考虑输出层 $O$，针对该层每一个节点的值 $a^{O}$，都由 $O-1$ 层参数而来，如下计算得到：
    
    $$
    a_j^{(O)}=\sigma(Z^{(O)})=\sigma[\sum_i (w_i^{(O-1)}\cdot a_i^{(O-1)}+b_i^{(O-1)})]
    $$
    
    并且，该层的损失值经过如下计算得到：
    
    $$
    C_O=\sum_j(a_j^{(O)}-e_j)^2
    $$
    
    1. 我们考虑调整权重 $w$，计算如下梯度：
        
        $$
        \nabla(C)=\frac{\partial C}{\partial w_{ij}}= \frac{\partial C}{\partial a} \frac{\partial a}{\partial Z} \frac{\partial Z}{\partial w}=2(a_j^{(O)}-e_j)\cdot \sigma^\prime(Z^{(O)})\cdot a_i^{(O-1)}
        $$
        
    2. 我们考虑调整偏置 $b$，计算如下梯度：
    
    $$
    \nabla(C)=\frac{\partial C}{\partial b_{ij}}= \frac{\partial C}{\partial a} \frac{\partial a}{\partial Z} \frac{\partial Z}{\partial b}=2(a_j^{(O)}-e_j)\cdot \sigma^\prime(Z^{(O)})
    $$
    
    由此，我们得到了该层的梯度值，组成梯度下降向量的一部分，我们逐层计算，最终得到如下式中的整个偏导向量。我们朝向梯度的反方向，改变网络中的权重与偏置：
    
    $$
    \left(\begin{array}{c}w_{1} \\ b_{1} \\ \vdots \\ w_{n} \\ b_{n}\end{array}\right)=\left(\begin{array}{c}w_{1}^{\prime} \\ b_{1}^{\prime} \\ \vdots \\ w_{n}^{\prime} \\ b_{n}\end{array}\right)+(-1)\cdot\left[\left(\begin{array}{c}\frac{\partial c}{\partial w_{1}} \\ \frac{\partial c}{\partial w_{n}} \\\vdots \\ \frac{\partial c}{\partial w_{n}} \\ \frac{\partial c}{\partial b_{n}}\end{array}\right)+\cdots\right]\cdot \frac{1}{n}
    $$
    
    其中，我们每 $n$ 次试验取平均梯度下降量，然后更新网络内权重。这种方法叫做随机梯度下降，而不是将所有实验均做完，再统一求均值得到参数下降量，加快计算速度。