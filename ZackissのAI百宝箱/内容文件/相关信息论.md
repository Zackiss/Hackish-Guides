# 相关信息论

### 人工智能

- 定义
    
    人工智能包括大部分致力于拟合人类智能结果的相关算法，机器学习是人工智能的一个分支，致力于利用经验知识自动优化算法，而深度学习一般指以受人脑启发的方式处理数据。
    
- 优点
    
    人工智能与人力智能相比的优点如下：
    
    - 与人类相比犯错更少
    - 能胜任对人而言更具有危险性的工作
    - 可以替代人类做重复性的工作
    - 工作时长更长
    - 可以胜任电子助理的角色
    - 可以快速产生决策
    - 可以在很多方面解决一些具有启发性的复杂问题
    - 加速新发明，新应用的产生
- 缺点
    
    人工智能与人力智能相比的缺点如下：
    
    - 设计与训练人工智能的成本很高
    - 取代人类的工作，造成一定程度的失业
    - 使人不再勤奋
    - 缺乏团队协作能力，没有人类的内在情感，缺少与人的纽带
    - 目前为止，人工智能不能得出一些创新性（Out of box）的思想
- 工程师的基本能力
    - 数学能力
        - 离散数学
        - 微积分
        - 概率与统计
    - 主流语言编程能力
    - 流程理解与算法设计
    - 数据分析背景
    - 机器学习专业知识
- 人工智能历史
    - 1952-1956
        - 人工智能的诞生
        - 图灵机的发明，图灵提出了赋予机器智能的问题
        - 图灵测试，30%分辨目标界定机器智能程度
    - 1956-1974
        - 符号化的人工智能
        - 约翰·麦卡锡提出了人工智能的概念
        - 亚瑟·塞缪尔设计了一系列人工智能算法
        - 第一个 NLP 模型 ELIZA 诞生，用于心理治疗
    - 1974-1980
        - 第一次人工智能寒冬
    - 1980-1987
        - 技术爆炸
        - 爱德华·费根鲍姆设计专家系统
        - IBM 设计深蓝国际象棋程序击败象棋大师
    - 1987-1993
        - 第二次人工智能寒冬
    - 1993-2011
        - 成功达成一些基本的人工智能应用目标
        - iRobot 发明 Roomba 扫地机器人
        - Google 发明自动驾驶汽车解决城市问题
        - IBM 的 Watson 超级计算机在电视节目上击败人类选手
    - 2011-今天
        - 深度学习与大数据的结合，泛化人工智能场景
        - 伊恩·古德费洛在 2014 年设计对抗性神经网络（GAN）
        - AlphaGo 战胜人类围棋选手
- 人工智能应用
    - 相关库
        - Tensorflow
            - Google 发明
            - 相对复杂
        - Pytorch
        - Scikit-learn
        - Keras
            - 基于 Tensorflow
            - 相对易用，相对容易拓展
    - Python
        - 优点
            - 易用，易读写
            - 容易与其他语言配合使用
            - 人工智能库丰富
            - 流行，社区比较大
        - 缺点
            - 速度有限
            - 不适合移动设备
            - 不适合游戏开发
            - 设计限制
        - 速度问题
            - 注意使用数组代替循环
            - 注意使用一些数学库加速运算

### 信息熵

- 信息量
    
    对于一个概率为 $p$ 的概率事件而言，我们都有一个信息（事件结果）。信息的获知完全消除了该概率事件的不确定性，使这个概率事件确定下来，这便是信息的意义所在。
    
    为了衡量信息的价值，我们需要一个量，由于信息和它背后的概率事件绑定，因此信息量应该是一个关于 $p$ 的函数。
    
    直观来看，信息确定下来的事件原本的概率越小，信息量就越大。
    
    在猜测骰子点数的案例中，“点数大于 $3$ ”的信息量要比“点数为 $5$ ”的信息量小，因为后者的内容将一个概率相对小的事件确定了下来。
    
    有一个信息，消除一个发生概率为 $p$ 事件的不确定性，这个信息拥有的信息量如下：
    
    $$I=-\log_{\text{unit}}p$$    
    - 为什么是对数形式，而不是其他的非线性关系？
        
        设 $I(p)$ 表示事件 $A$ 的信息量，$p$ 为事件 $A$ 发生的概率，如果两个事件 $A,\ B$ 同时发生，概率应为 $p=p_1\cdot p_2$，由于多个事件同时发生的总信息量应该等于每个事件信息量的和，因此，对应的信息量应该是和的形式 $I(p)=I(p_1)+I(p_2)$。
        
        由此，不难看出信息量应该符合 $f(x\cdot y)=f(x)+f(y)$ ，因此 $I(p)$ 应是对数函数。
        
        - 对于唯一性，我们利用上述性质可以做出证明
            
            设 $x=y=1$，我们得到 $f(1)=0$，且由微积分基本定理，我们有 $f(1)-f(x)=\int^1_xf^\prime(t)dt$，反复利用 $f(x\cdot y)=f(x)+f(y)$ 重构该积分，最终我们得到：
            
      $$-1\cdot f(x)=f(1)-f(x)=\int^1_xf^\prime(t)dt= f^\prime(1)\int^1_x\frac{1}{t}dt=C\cdot \ln(x)$$            
- 信息系统的熵
    
    现在我们有 $n$ 个信息，是 $n$ 个独立概率事件的结果。如果我们把这些概率事件看作一个系统，那么这个系统的信息量 $I$ 就是这些事件信息量 $I_i$ 的均值，$I$ 即为系统的信息熵。
    
    总的来说，当一个系统内发生一个事件时，这个事件的信息量的期望即是信息熵：
    
    $$H=\sum p_i \log(p_i)$$    
    另一方面，信息熵也能反应出信息系统的混乱程度（复杂程度）：
    
    如果系统内各个事件的概率相近，那么我们很难确定哪一个事件确实发生。（这也印证了一旦获知某个事件结果，信息量很大），反之，如果系统内一个事件的概率异常大，其余事件概率异常小，那么平均信息量就很低，我们有充分自信确定大概率事件发生，小概率事件不发生。
    
    这里有一个问题，虽然小概率事件结果的信息量很大，但是它在系统中发生的概率很小，因此对信息熵的贡献并不大。（ $x\cdot \log(x)$ 为一个凸函数）
    
    对一个概率系统 $P$，我们有系统的信息熵：
    
    $$H(P):=E(P_f)=-\sum_{i=1}^mp_i\cdot \log_2p_i\sim\int_{-\infty}^{+\infty}p(x)\cdot \log_2p(x)$$    
    其中，$p_i$ 为离散分布的概率值，$p(x)$ 为连续分布的概率密度函数。
    
- 相对熵（KL 散度）
    
    为了衡量处于两个不同分布的相似程度，我们找到两个分布的共性，信息熵。
    
    一般而言，两个不同分布的信息熵越接近，那么两个分布越相似。
    
    以分布 $P$ 为基准，考虑分布 $Q$ 与分布 $P$ 在熵意义上的差值。
    
    $$\begin{aligned}D_{KL}(P\Vert Q)= &\sum_{i=1}^mp_i\cdot(I_q-I_p)\\=&\sum_{i=1}^mp_i\cdot [-\log_2q_i-(-\log_2p_i)]\\=&\sum_{i=1}^m(p_i\cdot -\log_2q_i)-\sum_{i=1}^m(p_i\cdot-\log_2p_i)\\=& \sum_{i=1}^m p_i\log_2\frac{p_i}{q_i}\end{aligned}$$    
    当二者相对熵为 $0$ 时，$Q$ 和 $P$ 最接近。否则二者在 $p_i$ 位置上，信息量存在 $I_q-I_p$ 的差距。
    
    考虑到 $D_{KL}(P\Vert Q)$ 的正负问题，我们有吉布斯不等式：$\sum_{i=1}^m(p_i\cdot -\log_2q_i)\geq\sum_{i=1}^m(p_i\cdot-\log_2p_i)$，因此 $D_{KL}(P\Vert Q)\geq 0$，其值越接近 $0$，那么两个分布越接近。
    
    当其作为神经网络损失函数时，神经网络有理想输出 $x_i=p_i\in \{0,1\}$，真实输出 $y_i=q_i\in[0,1]$，对于总量为 $n$ 的输入（Batch），每一个输入都有两种输出的相对熵，我们对其求和。
    
    我们要使真实输出与理想输出尽可能服从相同的概率分布，计算两分布相对熵：
    
    $$\begin{aligned}H&=\sum_{m=1}^{n}\sum_{i=1,2}p_i\cdot(-\log_2q_i)-0\\&= \sum_{m=1}^{n} x_1\cdot \log_2 y_1+x_2\cdot \log_2(y_2) \\&= \sum_{m=1}^{n} x_1\cdot \log_2y_1+(1-x_1)\cdot \log_2(1-y_1)\end{aligned}$$    
- 交叉熵
    
    从相对熵的定义中，一般后项均没有意义，我们抽出有意义部分 $\sum_{i=1}^m(p_i\cdot -\log_2q_i)$，定义其为交叉熵，记作：
    
    $$\mathbb E_{p_i}(-\log q_i)= \sum_{i=1}^m(p_i\cdot -\log_2q_i)$$    
    其中，$\mathbb E_p(q)$ 为基于 $p$ 之上，$q$ 的期望大小。
    

### 损失函数

<aside>
💡 优化模型时，可以构造损失函数，逼近其最小值，使模型拟合给定数据

</aside>

- 随机梯度下降
    
    对于损失函数 $J(w)$，梯度下降是通过 $J(w)$ 对 $w$ 的一阶偏导数来找下降方向，令 $w$ 向梯度下降方向移动，并且以迭代的方式来更新 $J(w)$ 中的参数：
    
    $$g_i = \frac{\partial J(w)} {\partial w_i} \\ w^{k+1}_i=w^k_i-\alpha g_i$$    
    其中，$k$ 为迭代次数，在迭代过程中，我们可以指明深度 $k_{max}$，也可以检查 $||J(w^{k+1})−J(w^k)|| < l$ 是否达到局部最值来结束参数迭代。
    
- 牛顿法
- 正则化法
    - L1 正则化
        
        
    - L2 正则化
        
        
    - 正则方法区别