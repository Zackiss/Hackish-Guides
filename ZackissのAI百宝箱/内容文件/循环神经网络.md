# 循环神经网络

<aside>
💡 循环神经网络补足了卷积神经网络处理序列资料的短板

</aside>

### 网络结构

- 时序输入分析
    
    在处理上下文有关的数据时，可以利用循环神经网络。循环神经网络考虑了单个隐藏层在时序上的变化，考虑了隐层的神经元在时间上的成长。
    
    对于时间 $t$ 下的隐藏层 $X$ 而言，我们有 $X_{t-1}, X_{t}, X_{t+1}$ 三个不同时刻的隐藏层。我们在这三个隐藏层的中间神经元间构建网络，于是有层级间权重矩阵，记为 $W_{s_{t-1, t}}, W_{s_{t,t+1}}$
    
    我们注意到对于隐藏层 $X$ 而言，在考虑时间时的不同时刻隐藏层间相连（尤其是自相连，也可以是全连接），等同于隐藏层 $X$ 中神经元的自相连，即图的循环。为了减少参数量，我们一般令不同时序的权重矩阵共享参数，记为 $W_s$。
    
    对于一般多层感知机，我们有隐藏层 $X_i$ 的输出 $s_i$：
    
    $$
    s_i=f(\sum_n^N(w_{in}^i\cdot x_n^i+b_i))
    $$
    
    记作矩阵形式，为如下所示：
    
    $$
    S=f(W_{in}X+b)
    $$
    
    在循环神经网络中，我们添加了时序自相连的额外参数，如下所示影响输出：
    
    $$
    S_t=f(W_{in}X+W_sS_{t-1}+b)
    $$
    
    这样就增加了每次输入对于前一次输入的记忆，使得卷积神经网络有了处理时序输入的能力。在CV层面，可以以时间为序的每一张动画帧图作为按顺序的输入，在NLP层面，可以训练有上下文逻辑的语句，按顺序输入句中词，使得对词的含义的理解不再孤立，而是结合先前输入。
    
- 缓解过拟合
    
    <aside>
    💡 循环神经网络采用了 DropOut 机制缓解了过拟合的问题
    
    </aside>
    
    DropOut 机制是指在训练过程中，按照一定比例随机舍弃掉一部分的神经元连接，训练缺省模型。需要注意的是，在训练结束后，在模型使用阶段，训练中缺省部分的连接并未消失，而是依然保留在同一个模型中。因此其只是在训练的时候训练一小部分不同的神经元。
    
    DropOut 机制是 Bagging 机制的一种，Bagging 机制是指用同一数据集训练不同模型，再进行模型融合（例如最简单的融合方式：模型投票或取平均值）而DropOut机制的本质是通过断开神经元连接，将大模型缩小参数规模，训练大模型的一部分（也可以看作是 Bagging 中提及的待融合的分散小模型），再将这些部分融合的过程。
    
    这种机制可以磨消掉神经元间的共适应（Co-adaptation），即节点之间的固定位置关系带来的信息。因为每一次断开的神经元均不同，网络权重的更新也就不会依赖于节点之间的固定关系，因此能学习到更泛化的特征，也就是每个神经元不会对另一个特定的神经元更敏感。
    
    在循环神经网络中，如果单纯剪裁整个循环神经网络中的神经元连接，可能会导致记忆力减退。有研究发现，时序循环中，随着时间增长，其中的信息杂质也会越来越多。
    
    因此，我们有以下剪裁方式：
    
    - 只对前馈的多层感知机结构中的连接进行随机裁剪，而不裁剪循环运算的时序连接部分。
    - 利用变分推理，对于同一个训练集，所有时序间连接均采用相同的裁剪方式，而不是随机选取，对于不同时刻的舍弃是相同的。一般而言，这种方式能更好地工作于NLP范畴。
    - 利用 ZoneOut 结构，剪裁依然随机选取，但不再是直接将连接权重归零，而是不对下一个时序更新该权重（保持上一个时序中的权重初始值）
- 长期依赖问题
    
    网络层数增大，可能会发生误差或梯度消失，爆炸的情况，使得无法继续优化。这个问题在循环神经网络中非常的显著，因为时序过长，循环的权重就会过大，或过小。在NLP层面，循环神经网络因为其循环的设计，使其容易忘记很久之前的输入，对于过长的句子输入，很容易直接丧失句子的前半部分信息。
    
    对于循环神经网络的隐藏层，基于时序，我们有：
    
    $$
    S=f(\cdots f(W_{t_1}\cdot f(W_{t_0}X+b)+b\cdots)
    $$
    
    不考虑激活函数，我们有：
    
    $$
    S_t=W^{t-t_0}S_{t_0}
    $$
    
    对于矩阵高次幂运算，强度小于 $1$ 的元素会趋于 $0$，强度大于 $1$ 的元素会趋于 $+\infty$，这导致了梯度爆炸或消失的现象。
    
    考虑反向传播，我们有损失函数 $L$：
    
    $$
    \frac{\partial L_t}{\partial h_{t_0}}=\frac{\partial L_t}{\partial h_{t}}\cdot \frac{\partial h_t}{\partial h_{t-1}}\cdots \frac{\partial L_{t_0+1}}{\partial h_{t_0}}=\frac{\partial L_t}{\partial h_{t}}W^{t-t_0}
    $$
    
    又一次出现了同一矩阵的高幂次运算，同上会出现相似的问题。
    
    正因为循环神经网络将不同时序的权重矩阵 $W_t$ 共享了参数，只用同一个 $W$ 来代替，才出现了矩阵的高幂次运算，因此更容易出现上述的问题。
    
    考虑到激活函数，如果激活函数是类似于 $\text{ReLu}$ 的函数，依然无法缓解该问题。但在反向传播过程中，涉及到激活函数的导数，如果激活函数的导数值始终不超过 $1$，可能缓解梯度爆炸的问题。
    
    对于这个问题，我们可以在时序上采用类似于 $\text{ResNet}$ 的跳跃连接，缓解这个问题。我们也可以使用长短期记忆网络，门控循环单元来解决。