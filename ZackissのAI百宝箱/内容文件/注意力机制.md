# 注意力机制

### Transformer 模型

- 模型思路
    
    由于传统的CNN，RNN模型局限于从左向右或从右向左依次计算，因此时间 $t$ 的计算结果严格依赖于时间 $t-1$ 的计算结果，从而丢失了模型的并行运算能力，并且在顺序计算中，对久远计算结果的长期记忆很容易丢失。基于这些考虑，模型参考了人眼的注意力机制，设计出本模型。
    
    这是一个由编码器（Encoder）与解码器（Decoder）组成的模型，直观理解为利用编码器与解码器提取输入语句的特征，并利用得到的原语句中词语的特征值，学习目标语句中词语的概率，训练过程与传统神经网络相似，依然使用求解损失函数最小值的办法，使模型最大拟合，但利用特征作为词语在学习中的权重占比这一点与以往不同。
    
- 编码器
    
    编码器主要由多头注意力机制，残差机制，感知机模型构成：
    
    - 注意力模型
        
        $$
        Z=\text{attention}(Q,K,V)=\text{softmax}(\frac{QK^T}{\sqrt{d_k}} )\cdot V
        $$
        
        - 注意力过程
            
            输入句子由 $N$ 个词向量 $X_i$ 组成，大致为 $S_N=[X_1,X_2,\cdots,X_n]$ 的结构。
            
            我们有一个引导词，还有一列组成句子的词语，而我们的目的是对某个词语来讲，获得每个词语对它而言，重要性的权重：
            
            $$
            S_N=[X_1,X_2,\cdots,X_n]\rightarrow P_N=[p_1,p_2,\cdots,p_n]
            $$
            
            1. 选定一个词语，通过预训练模型获得对应的词向量 $I_0$，句子也进行分词，向量化处理。
            2. 随机初始化三个矩阵 $W_q$，$W_k$，$W_v$，这些矩阵会训练优化（比如梯度下降），利用这三个矩阵，我们从 $S_N$ 中取出词向量 $a_i$，进行线性变换，得到特征向量 $Q$，$K$，$V$：
                - $Q=I_0\cdot W_q$，该特征向量作为 $\text{Query}$（提问值）
                - $K=a_i \cdot W_k$，该特征向量作为 $\text{Key}$（索引值）
                - $V=a_i\cdot W_v$，该特征向量作为 $\text{Value}$（词语值）
                
                其中，在某些模型中，词语值变换 $W_k$ 与索引值变换 $W_v$ 被设定成同一矩阵。
                
            3. 通过点乘计算向量相似度，得到提问词与句中词的相关程度：
                
                $$
                \text{similarity}=(Q\cdot K)\cdot V/\sqrt{d_{model}}
                $$
                
                其中，除以向量维度 $d_{model}$ 是为了防止相似度数值过大的归一化处理，乘以词语值 $V$ 是将句中词本身在预训练模型中所揭示的重要性也计入其中。
                
        - 自注意力机制
            
            除了得到提问词与某一词语的关联程度之外，推广而言，我们还可以通过这种计算相关性的办法，计算句中某一个词语在整句中的重要性。我们认为，一个词在句中如果与其他所有自身权重很大的词语的关联程度都非常大，并且它自身的权重也非常大，这个词语就很重要。
            
            这次我们从句中取出提问词，计算它与句中其他词语的相似度，找到 $S_N$ 中所有词向量与选定词的关联程度之和：
            
            $$
            \text{compatibility}=\sum \text{weight}\cdot \text{value}
            $$
            
            加权和的权重由 $\text{Key}$ 和 $\text{Query}$ 的相似度得到，计算中 $\text{Value}$ 与上文提及的词语值相同。
            
            因此，加入激活函数后，我们将此称之为自注意力过程：
            
            $$
            {P_N}_i= \sum \text{weight}\cdot \text{value}=\sum \text{softmax}(\frac{Q_i K_i^T}{\sqrt{d}} )\cdot V_i
            $$
            
            $$
            Y=\text{softmax}(X)： \ Y_i=\frac{e^{X_i}}{\sum_{i=1}^Ne^{X_j}},\ (i \in \{1, 2, \cdots N\})
            $$
            
            除了通过向量点乘的方式得到句中重要性算法的权重，我们还可使用一个前馈（FeedForward）神经网络 $\text{FFN}$ 结构，训练两词相关性，得到权重参数：
            
            $$
            {P_N}_i= \sum \text{weight}\cdot \text{value}=\sum\text{FFN}(I,X_i)\cdot V_i
            $$
            
            结束计算时，$N$ 个词语对应 $N$ 个句中重要程度，逐个对应保存在 $P_N$ 中。
            
        - 多头自注意力机制
            
            这是一对于 $W_q$，$W_k$，$W_v$ 的设计，我们认为单一的 $W_q$，$W_k$，$W_v$ 并不能完整提取到词语的全部特征，不能将全部特征融合到注意力。因此我们设计多组 $W_q$，$W_k$，$W_v$，如下：
            
            $$
            \text{multi-head}=\{\{{W_1}_q,{W_1}_k,{W_1}_v\}, \{{W_2}_q,{W_2}_k,{W_2}_v\},\cdots\}
            $$
            
            对每一个词向量 $a_i$，对于每一组变换矩阵，我们分别进行特征变换，得到如下结果：
            
            $$
            \text{trans-set}_{a_i}=\{\{Q_1,K_1,V_1\}, \{Q_2,K_2,V_2\},\cdots\}
            $$
            
            进行一般注意力过程，我们得到多个重要性矩阵，合并（Concat）为一个矩阵：
            
            $$
            Z_i=\text{attention}(Q_i,K_i,V_i)=\text{softmax}(\frac{Q_iK_i^T}{\sqrt{d_k}} )\cdot V_i
            $$
            
            $$
            \text{attent-mat}_{a_i}=\left(\begin{array}{cccc}Z_1\\Z_2\\\vdots\\Z_n\end{array}\right)
            $$
            
            传入一个全连接层，将此长矩阵压缩为一个较小的矩阵：
            
            $$
            Z_{a_i}=\text{FFN}(\text{attent-mat}_{a_i})
            $$
            
    - 残差机制
        
        来自于 $\text{ResNet}$，为了解决神经网络训练的退化问题，直接将输入与该层输出进行一个线性组合。在该模型中，无论是双层感知机，还是注意力模型，都采用了相似的做法。
        
    - 前馈全连接模型
        
        双层全连接模型，第一层的激活函数使用了 $\text{ReLu}$，第二层使用了线性激活函数：
        
        $$
        \text{FFN}(Z)=W_2\cdot \max(0,W_1\cdot Z+b_1)+b_2
        $$
        
    
    整体来看，编码器（Encoder Set）有 $N=6$ 个编码块（Encoder），对每一个编码块而言，它分别包含如上两个模型，如下以第一个编码块举例：
    
    1. 获得句子中每个词的词向量 $x_i$，位置？？？
    2. 利用多头注意力模型，将这些词向量分别转化为对应的重要性矩阵 $z_i$，这时候词语本身的信息已经得到了提取，但词语之间并不存在训练过程：
        
        $$
        \{x_1,x_2,\cdots x_n\}\rightarrow\text{FFN}_1(\text{attent-mat}_{a_i})\rightarrow \{z_1,z_2,\cdots z_n\}
        $$
        
    3. 将数个 $z_i$ 传入同一个双层感知机，训练词间特征，得到多个输出 $r_i$，作为下一个编码块输入：
        
        $$
        \{z_1,z_2,\cdots z_n\}\rightarrow\text{FFN}_2(Z_N)\rightarrow\{r_1,r_2,\cdots r_n\}
        $$
        
    4. 第一个编码块输入需要词向量化，向量维度选定为 $d_{model}=512$，接着，上一个编码块的输出作为下一个编码块的输入，最后一个编码块的输出传入所有的解码器中，如下图所示：
        
        ![Untitled](%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%201aaa1b87fdb945a1aba4617b35382513/Untitled.png)
        
- 解码器
    
    解码器的目的是通过给编码器给定的句子特征，预测目标句子。
    
    解码器和编码器的结构基本相同，但在注意力模型与双层全连接模型之间，多了一层额外的注意力模型（Encoder-Decoder attention），因此我们有：
    
    $$
    \begin{aligned}\{r_1,r_2,\cdots r_n\}&\rightarrow\text{FFN}_1(\text{attent-mat}_{a_i})\\&\rightarrow\text{En-De-attention}(Q_D,K_E,V_E)\\&\rightarrow \text{FFN}_2(Z_N)\end{aligned}
    $$
    
    对于这个一般的注意力模型，采用了遮罩（Masked）机制，即第 $k$ 个解码器只依赖于它之前所有的解码器的输出，不依赖于它之后（$k_i>k$）的解码器。
    
    对于这个多出的注意力模型，它的提问词特征 $Q$ 来自于上一个解码块，而它的句中词特征 $K,V$ 来自于顶层编码器的输出，辅助解码器注意那些编码器带来的信息的关键位置：
    
    $$
    Z=\text{En-De-attention}(Q_D,K_E,V_E)=\text{softmax}(\frac{Q_DK_E^T}{\sqrt{d_k}} )\cdot V_E
    $$
    
    全连接层不变，依然是一个双层感知机。
    
    最后，将解码器顶层输出接入 $\text{FFN}_2(D_{\text{out}})$，得到可能的目标句子所有词语的概率。我们利用 $\text{CTC}$ 时序分类损失函数（Connectionist temporal classification）训练整个模型的特征提取矩阵和所有的感知机结构。
    
- 参考链接
    
    [详解Transformer （Attention Is All You Need）](https://zhuanlan.zhihu.com/p/48508221)
    
    [Self-Attention和Transformer](https://luweikxy.gitbook.io/machine-learning-notes/self-attention-and-transformer)
    

### BERT 架构

### GPT 架构