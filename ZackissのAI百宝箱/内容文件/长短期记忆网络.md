# 长短期记忆网络

<aside>
💡 长短期记忆网络（LSTM）缓解了循环神经网络的梯度爆炸、消失问题

</aside>

- 网络结构
    
    循环神经网络只记得前一时刻的数据特征，即只具有短期记忆：
    
    $$\cdots\rightarrow S_{t-1}\rightarrow S_t\rightarrow S_{t+1}\rightarrow \cdots$$    
    而长短期记忆网络，在循环神经网络中引入了一条长期记忆的时间序列：
    
    $$\begin{aligned}&\cdots\rightarrow C_{t-1}\rightarrow C_t\rightarrow C_{t+1}\rightarrow \cdots \\ &\cdots\rightarrow S_{t-1}\rightarrow S_t\rightarrow S_{t+1}\rightarrow \cdots\end{aligned}$$    
    其中，$C_t$ 与 $S_t$ 之间存在关系，如下有两个关系组成：
    
    - 记忆擦除（Forget Gate）：
        
        $$
        f_1=\sigma(W_1\cdot \begin{bmatrix}
        S_{t-1} \\
        X_{t}
        \end{bmatrix}+b_1)
        $$
        
        其中，$X_t$ 为该隐藏层的前馈输入，$S_{t-1}$ 为短期记忆（循环神经网络中的时序）
        
        将 $C_t$ 与 $W_1\cdot\begin{bmatrix}
        S_{t-1} \\
        X_{t}
        \end{bmatrix}$ 逐项相乘，抹去（选择性遗忘）了 $C_t$ 中 $W_1$ 取值为 $0$ 的元素。
        
    - 记忆添加（Input Gate）：
        
        $$
        f_2=\sigma(W_2\cdot \begin{bmatrix}
        S_{t-1} \\
        X_{t}
        \end{bmatrix}+b_2)\cdot \tanh(\hat W_2\cdot \begin{bmatrix}
        S_{t-1} \\
        X_{t}
        \end{bmatrix}+\hat b_2)
        $$
        
    
    因此，长期记忆在 $t$  时刻总的输出为：
    
    $$C_t=C_{t-1}\odot f_1+f_2$$    
    $t$  时刻隐藏层的输出为：
    
    $$\begin{aligned}S_t&=\sigma(S_{t-1})\cdot \tanh(C_t)\\&=\sigma(S_{t-1})\cdot \tanh(C_{t-1}\cdot f_1(S_{t-1})+f_2(S_{t-1}))\end{aligned}$$    
    我们继续解析这种复杂的长期记忆、短期记忆相互干预关系与对输出的干预关系。
    
- 门控设计
    
    关于门的含义，在这里门是指对一矩阵中所有元素分别乘以 $[0,1]$ 之间的值，进行数据处理。在这里，门的权重可以不只是 $0, 1$，也可以是一个 $[0,1]$ 之间的值，因此所有门的函数我们都选用了 $\text{Sigmoid}$ 函数。
    
    - 遗忘门（Forget Gate）
        
        观察影响长期记忆的第一步，也就是 Forget Gate 的设计，我们发现：
        
        $$
        f_1=\sigma(W_1\cdot \begin{bmatrix}
        S_{t-1} \\
        X_{t}
        \end{bmatrix}+b_1)
        $$
        
        这一部分即将用于磨削长期记忆，$c_t= f_1\odot c_{t-1}$
        
        不难理解，我们利用 $\text{Sigmoid}$ 函数，将短期记忆与输入的增广矩阵$A=\begin{bmatrix}
        S_{t-1} \\
        X_{t}
        \end{bmatrix}$中所有元素的大小压缩到 $[0,1]$ 区间内。然后，接近 $0$ 的元素与 $C_t$ 对应元素逐项相乘，大幅减小长期记忆中对应值。接近 $1$ 的元素小幅减小长期记忆中对应值。
        
    - 输入门（Input Gate）
        
        对于影响长期记忆的第二步，也就是 Input Gate 的设计，我们发现：
        
        $$
        f_2=\sigma(W_2\cdot \begin{bmatrix}
        S_{t-1} \\
        X_{t}
        \end{bmatrix}+b_2)\cdot \tanh(\hat W_2\cdot \begin{bmatrix}
        S_{t-1} \\
        X_{t}
        \end{bmatrix}+\hat b_2)
        $$
        
        这一部分是调整长期记忆，可能增加也可能减小，$C_t=C_{t-1}\odot f_1+f_2$
        
        首先利用 $\text{Sigmoid}$ 函数，确定长期记忆中每一元素的增幅，接着利用值域为 $[-1,1]$ 的$\text{Tanh}$ 函数确定该元素是增加还是减小。
        
        另外，一般来说，利用 $\text{Tanh}$ 函数的好处还有使得训练中梯度收敛得更快：
        
      $$\tanh^\prime(x)=1-\tanh^2(x)$$        
    - 输出门（Output Gate）
        
        最后隐藏层的输出为更新后的短期记忆，由前一刻短期记忆与此刻的长期记忆组成：
        
      $$\begin{aligned}S_t&=\sigma(S_{t-1})\cdot \tanh(C_t)\\&=\sigma(S_{t-1})\cdot \tanh(C_{t-1}\cdot f_1(S_{t-1})+f_2(S_{t-1}))\end{aligned}$$        
         $\text{Tanh}$ 的设计使长期记忆对于短期记忆的影响更灵活，但可能不是必要的。
        
- 门控循环单元（GNU）