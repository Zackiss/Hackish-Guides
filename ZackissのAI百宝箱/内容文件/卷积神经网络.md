# 卷积神经网络

### 网络结构

- 卷积过程
    
    <aside>
    💡 卷积过程考虑了数据的连续特征
    
    </aside>
    
    - 计算过程
        
        我们利用卷积核矩阵，在输入矩阵上进行一系列有序扫描运算，最终一个输入节点（多通道特征图数据）局部连接输出节点（卷积核），构成单层卷积网络。
        
        我们有一个 $m\times m$ 大小的特征数据矩阵 $A$ ，选定一个 $n\times n$ 大小的矩阵 $K$（卷积核）：
        
        $$
        K=\begin{pmatrix}  
          K_{1,1} & K_{1,2} & K_{1,3} \\  
          K_{2,1} & K_{2,2} & K_{2,3} \\  
          K_{3,1} & K_{3,2} & K_{3,3}  
          \end{pmatrix} \ n=3
        $$
        
        每一个卷积操作本质是一个点积过程，我们从 $A$ 中选定一个大小为 $n\times n$ 的分块子矩阵：
        
      $$A_{n\times n}=A[i,i+1,\cdots ,i+n;j,j+1,\cdots ,j+n]$$        
        通过逐项内积（Hadamard Product），我们得到输出特征矩阵的一个元素的值：
        
        $$
        O_{ij}=K*A_{n\times n}=\sum_{\text{entry}}\begin{pmatrix}  
          A_{1,1}\cdot K_{1,1} & A_{1,2}\cdot K_{1,2} & A_{1,3}\cdot K_{1,3} \\  
          A_{2,1}\cdot K_{2,1} & A_{2,2}\cdot K_{2,2} & A_{2,3}\cdot K_{2,3} \\  
          A_{3,1}\cdot K_{3,1} & A_{3,2}\cdot K_{3,2} & A_{3,3}\cdot K_{3,3}  
        \end{pmatrix} 
        $$
        
        接着我们将上一层输出矩阵作为下一层输入矩阵，形成一个层级结构。
        
        上述过程中，步长 $p=1$，即对正整数 $k\in [0, \lceil \frac{n}{2p}\rceil ]$，有 $i=k\cdot p+i_0,\ j=k\cdot p+j_0$ ，因此，我们的输出特征矩阵 $O$ 的大小为 $\lceil \frac{n}{2p}\rceil \times \lceil \frac{n}{2p}\rceil$，随着步长的增大逐渐减小。
        
        此外，如果 $A$ 的大小 $n<2\cdot \lceil \frac{n}{2p}\rceil-1$ ，我们需要边界填充（Padding）
        
        最终，我们得到一个大小为 $h\times w$ 的特征图，作为卷积层输出：
        
      $$\begin{aligned}h&=\frac{A_h-F_h+2P}{p}+1 \\ w&=\frac{A_w-F_w+2P}{p}+1\end{aligned}$$        
        其中，$A$ 代表输入矩阵，$F$ 代表卷积核，$p$ 表示步长，$P$ 表示边界填充的圈数。
        
        整个卷积过程相当于局部加权求和的过程，其中卷积核为权重矩阵，特征数据矩阵为上层节点，由此继承了多层感知机的理念。
        
        不过与多层感知机不同的是：
        
        1. 卷积加权求和过程只考虑相邻节点加权和，不同于多层感知机，全部子节点求加权和。
        2. 对于一层的所有节点而言，加权和的参数均为 $K$ 的元素值，因此连接权重均相同。
        3. 由于只考虑相邻节点的加权和，卷积网络保留了原始数据的结构性（连续的特征）
    - 计算感受野
        
        由于下一层特征图经上一层经卷积获得，因此下一层的值会受到上一层的某个区域影响，而上一层的值又受到上上层的影响。感受野的定义是对于某层输出特征图上的某个点，在卷积神经网络的原始输入资料上能影响到这个点的设定值的区域大小。
        
        若第 $i$ 层为卷积层或池化层（Pooling Layer），那么有：
        
      $$R_e^{(i)}=\min\left( R_e^{(i-1)}+(k_e^{i}-1)\prod p \right)$$        
- 激活函数
    
    <aside>
    💡 激活函数将卷积得到的值变为非线性结果
    
    </aside>
    
    不同于逻辑斯蒂函数与反正切函数，我们使用简单的分段线型函数 Relu 来作为激活函数：
    
    $$f(x)=\max{(x,0)}$$    
    因为卷积过程只是一个线性变换过程，我们需要对输出引入一个非线型函数，使得卷积网络可以拟合非线性的函数。
    
- 池化过程
    
    <aside>
    💡 池化过程是一个压缩特征信息的过程
    
    </aside>
    
    池化过程是对卷积层的输出矩阵分子矩阵筛选，构成更小的输出矩阵的过程。
    
    在此以最大池化为例，我们有输出矩阵 $O$，选定分块子矩阵 $I_{i,j}$，原矩阵表示为：
    
    $$O= \left(\begin{array}{cccc}I_{1,1} & I_{1,2} & \cdots & I_{1,n} \\ I_{2,1} & I_{2,2} & & I_{2,n} \\ \vdots & \vdots & \ddots & \vdots \\ I_{k,1} & I_{k,2} & \cdots & I_{k,n}\end{array}\right); \ p=2,\ n=2$$    
    对每一个子矩阵 $I_{i,j}$，取其最大元 $a$，作为更小的输出矩阵的元：
    
    $$a=\max_{m,n}{(I_{i,j})}_{m,n}$$    
    $$O_p= \left(\begin{array}{cccc}a_{1,1} & a_{1,2} & \cdots & a_{1,n} \\ a_{2,1} & a_{2,2} & & a_{2,n} \\ \vdots & \vdots & \ddots & \vdots \\ a_{k,1} & a_{k,2} & \cdots & a_{k,n}\end{array}\right)$$    
- 权值更新
    
    在卷积神经网络中，权值更新即意为卷积核的参数更新，其余超参数需要经过手动调节。与多层感知机更新权值相同，我们采用反向传播，梯度下降的链式回传，更新卷积核内的参数。
    
    损失函数
    

### 参量计算

- 感受野
    
    <aside>
    💡 感受野反映了原始输入对某层输出的神经元的影响。
    
    </aside>
    
    感受野是神经网络某层输出的特征图（feature map）上的节点在原始输入上映射的区域大小。
    
    当层感受野大小受上一层感受野大小的影响，因此由递推计算得出：
    
    $$l_k = l_{k-1} + \left((f_k - 1) \cdot \prod_{i=1}^{k-1}s_i\right)$$    
    其中，$l_{k-1}$ 是 $k-1$ 层（即上一层）感受野的大小，$l_k$ 是当前层感受野的大小，$f_k$ 是当前层卷积核的尺寸，$s_i$ 是第 $i$  层的步长。
    
    该公式由如下而来：
    
    $$(N-1)_{RF} = f(N_{RF}, \text{stride}, \text{kernel}) = (N_{RF} - 1) \cdot \text{stride} + \text{kernel}$$    
    很容易理解，上一层的节点的感受野，是
    
- 输出尺寸
    
    

参数量与计算量

### 分组卷积

### 全卷积神经网络（FCN）

- U-Net
