# 行为分析

### 主成分分析

- 集群过程
    
    <aside>
    💡 将具有相似特征向量的样本放在同一类的方法
    
    </aside>
    
    - 构建特征向量
        - 具体操作步骤
            1. 返回按月重排的单独时间列
                
                ```python
                import pandas as pd
                
                x = df_info.resample('M').count()
                x = x.drop(x.columns.values, axis=1)
                
                ```
                
                - 相关解释
                    - `resample`方法
                        
                        操作`Dataframe`对象，按时间列重选采样频率，防止数据稠密
                        
                        在这里，`M` 代表按月重新采样
                        
                        返回一个 `Resampler` 对象
                        
                    - `count`方法
                        
                        操作`Resampler`对象，返回一个`Dataframe`对象，大致为如下格式
                        
                        | Date | Count |
                        | --- | --- |
                        | 2001-05-31 | 2 |
                        | 2001-06-30 | 3 |
                        | 2001-07-31 | 4 |
                        | 2001-08-30 | 1 |
                    - `drop`方法
                        
                        操作`Dataframe` 对象
                        
                        - `labels`：`List`类型，指定删除的行、列名
                        - `axis`：默认为`0`，删除行，更改为`1`时指定删除列
                        - `index`：指定删除的行索引
                        - `columns`：指定删除的列索引
                        - `inplace`：默认为`False`，删除不会改变原数据，返回`Dataframe`，更改为`True`，直接在原`Dataframe`上删除，无返回值
            2. 取出支付次数第一多，第二多的顾客ID值
                
                ```
                i_rank = 1
                j_rank = 2
                
                i_id = df_info['顧客ID'].value_counts().index[i_rank]
                j_id = df_info['顧客ID'].value_counts().index[j_rank]
                
                ```
                
            3. 取出所有对应顾客ID值的行，制作以该顾客每月使用次数为值的表格
                
                ```
                x_i = df_info[df_info['顧客ID']==i_id].resample('M').count()
                x_j = df_info[df_info['顧客ID']==j_id].resample('M').count()
                
                ```
                
            4. 处理缺失值
                
                ```
                x_i = pd.concat([x_0, x_i], axis=1).fillna(0)
                x_j = pd.concat([x_0, x_j], axis=1).fillna(0)
                
                ```
                
            5. 计算相似度
                
                <aside> 💡 以顾客每月使用次数为每维度值的特征向量
                
                </aside>
                
                ```
                sig_x = x_i.iloc[;,0].values
                
                ```
                
                ```
                dx = x_i.iloc[;,0].values-x_j.iloc[:,0].values # 两特征向量之差
                n = np.linalg.norm(dx) # 差向量长度（范数）
                sim = n/len(x_i) # 长度标准化到[0, 1]区间
                
                ```
                
                - 相关解释
                    
                    在`numpy`中，`pandas`的单列`Dataframe` 被看作高维向量，进行向量加减
                    
                    计算特征向量的距离，得到相似度，标准化越接近0，两组数据相似程度越高
                    
            6. 按照如上方法，计算支付次数前一百名的顾客特征向量
                
                <aside> 💡 通过上述对相似度的计算，基本确认了对于大客户而言，此种选取方式得到的特征向量相似度较高，推理出大客户一般符合这种行为逻辑
                
                </aside>
                
                ```
                list_vector = []
                total_num = 100
                for i_rank in range(total_num):
                    # 篩選出顧客ID
                    i_id = df_info['顧客ID'].value_counts().index[i_rank]
                    # 將每月使用次數設定為特徵值
                    x_i = df_info[df_info['顧客ID']==i_id].resample('M').count()
                    # 出現缺失值的處理方式
                    x_i = pd.concat([x_0, x_i], axis=1).fillna(0)
                    # 新增為特徵向量
                    list_vector.append(x_i.iloc[:,0].values.tolist())
                
                ```
                
    - 可视化特征向量（PCA）
        
        <aside>
        💡 PCA将分布在高维空间的特征向量降维，投影到低维（二维）观察
        
        </aside>
        
        - 具体操作步骤
            1. 进行PCA降维过程
                
                ```python
                from sklearn.decomposition import PCA
                import numpy as np
                import matplotlib.pyplot as plt
                # 轉換特徵向量
                features = np.array(list_vector)
                # 執行主成分分析
                pca = PCA()
                pca.fit(features)
                # 將特徵向量轉換成主成分
                transformed = pca.fit_transform(features)
                # 可視化
                for i in range(len(transformed)):
                    plt.scatter(transformed[i,0],transformed[i,1],color="k")
                    plt.text(transformed[i,0],transformed[i,1],str(i))
                plt.show()
                ```
                
                <aside>
                💡 分析分布特征，一般而言分布较为紧密的样本点，行为模式较为相似
                
                </aside>
                
            2. 将想要具体分析的样本顺位取出，观察日期折线图，分析是否如预期
                
                ```python
                import pandas as pd
                # 篩選出index
                x_0 = df_info.resample('M').count()
                x_0 = x_0.drop(x_0.columns.values,axis=1)
                
                # 其中22，25，42号顾客分布较近，设定顺位
                list_rank = [22,25,42]
                x = []
                for i_rank in list_rank:
                    # 篩選出顧客ID
                    i_id = df_info['顧客ID'].value_counts().index[i_rank]
                    # 將每月使用次數設定為特徵值
                    x_i = df_info[df_info['顧客ID']==i_id].resample('M').count()
                    # 出現缺失值的處理方式
                    x_i = pd.concat([x_0, x_i], axis=1).fillna(0)
                    # 繪製圖表
                    plt.plot(x_i)
                    plt.xticks(rotation=60)
                plt.show()
                ```
                
        - 数学原理分析
            - PCA降维
                
                降维的本质是对$n$个高维向量做对于$d$个基向量的投影：
                
      $$A=\left(a_1,a_2\cdots a_n\right)\xrightarrow[\left(p_1,p_2\cdots p_d\right)]{Remap}B=\left(b_1,b_2\cdots b_n\right)$$                
                而投影可以表示为点积，其中对于基向量，如果其为单位向量，那么点积的值即为投影的长度，故进行 $\vec{v} \cdot \vec{n}=Scal_nv$：
                
      $$\left(\begin{array}{c}p_{1} \\ p_{2} \\ \vdots \\ p_{d}\end{array}\right)\left(a_{1} , a_2\cdots a_{n}\right)=\left(\begin{array}{cccc}p_{1} a_{1} & p_{1} a_{2} & \cdots & p_{1} a_{n} \\ p_{2} a_{1} & p_{2} a_{2} & \cdots & p_{2} a_{n} \\ \vdots & \vdots & \ddots & \vdots \\ p_{d} a_{1} & p_{d} a_{2} & \cdots & p_{d} a_{n}\end{array}\right)$$                
                其中，$p_d$为$d$个基向量，$a_n$为$n$个原向量，$b_n$为映射之后的$n$个向量
                
            - 数据的向量化建模
                
                我们有多个用户的每月支付数据，進行建模。
                
                $\vec a_i = (x_i, y_i, z_i\cdots)$為單個用戶的每月支付數據：
                
      $$\left( \begin{array}{c}x_{1} \\ x_{2} \\ \vdots \\ x_{n}\end{array}\right)，\left( \begin{array}{c}y_{1} \\ y_{2} \\ \vdots \\ y_{n}\end{array}\right)，\left( \begin{array}{c}z_{1} \\ z_{2} \\ \vdots \\ z_{n}\end{array}\right)\cdots$$                
                我们将$\vec{x},\ \vec{y},\ \vec{z} \cdots$作为维度轴，得到特征向量$\vec a_i = (x_i, y_i, z_i\cdots)$
                
            - 基向量的选择
                - 最大可分性
                    
                    我们希望投影后的向量尽可能分散，由于线性相关性，越分散的向量包含的信息则越多，其信息熵也就越大，可分性也就越大。
                    
                - 方差
                    
                    根据如上的论述，我们需要找到一组基向量，使得映射后的向量包含的信息$x_i$的方差最大，也即为$x_i$（关于原点）的分布越分散。
                    
                    注意，方差是一维特征的性质。针对组成维度的各个基向量而言，对该基向量上不同样本的投影来进行计算，进而得到各个基向量上数据的方差。
                    
                    对于这个案例，某用户的所有月份支付次数表示为：
                    
      $$\vec{x}=\left( \begin{array}{c}x_{1} \\ x_{2} \\ \vdots \\ x_{n}\end{array}\right)$$                    
                    对于每一个特征，我们都有一个对应的方差值，对于
                    
                    我们有 $Base =\left( \vec{x},\vec{y},\vec{z}\cdots\right)$，那么$\vec{x}$的方差可表示为如下形式：
                    
      $$Var(\vec{x})=\frac{1}{n-1} \sum_{i=1}^{n} x_{i}^{2}$$                    
                - 协方差
                    
                    除了数据本身离散程度大之外，我们还需要数据之间两两无关，为了表示尽可能多的信息，我们需要向量的各个维度值之间保持线性无关，因此求解各个维度之间两两的协方差，使其值均为0。
                    
                    注意，协方差描述的是两个维度特征之间的无关性，我们计算两个基向量上不同样本的投影，得到这两个基向量上数据的协方差。
                    
                    对于这个案例，两个用户的所有月份支付次数表示为：
                    
      $$\vec{x}=\left( \begin{array}{c}x_{1} \\ x_{2} \\ \vdots \\ x_{n}\end{array}\right)，\vec{y}=\left( \begin{array}{c}y_{1} \\ y_{2} \\ \vdots \\ y_{n}\end{array}\right)$$                    
                    对于任意两个基向量上的投影数据，我们有协方差：
                    
      $$Cov(\vec{x}, \vec{y})=\frac{1}{n-1}\sum_{i=1}^{n} x_i y_i$$                    
                - 协方差矩阵
                    
                    我们将$n$个维度的数据同时进行计算
                    
                    首先，$X=(\vec{x}, \vec{y}, \vec{z}\cdots)^T$，为待降维的基向量方向投影数据
                    
      $$X = \left(\begin{array}{cccc}x_{1} & x_{2} & \cdots & x_{n} \\ y_{1} & y_{2} & & y_{n} \\ \vdots & \vdots & \ddots & \vdots \\ w_{1} & w_{2} & \cdots & w_{n}\end{array}\right)$$                    
      $$\frac{1}{n} X X^{T}=\left(\begin{array}{cccc}\frac{1}{n} \sum_{i=1}^{n} x_i^{2} & \frac{1}{n} \sum_{i=1}^{n} x_i y_i & \cdots & \frac{1}{n} \sum_{i=1}^{n} x_i w_i \\ \\ \frac{1}{n} \sum_{i=1}^{n} y_i x_i & \frac{1}{n} \sum_{i=1}^{r} y_i^{2} & \cdots & \frac{1}{n} \sum_{i=1}^{n} y_i w_i \\ \vdots & \vdots & \ddots & \vdots \\ \frac{1}{n} \sum_{i=1}^{n} w_i x_i & \frac{1}{n} \sum_{i=1}^{n} w_i y_i & \cdots &\frac{1}{n} \sum_{i=1}^{n} w_i^2\end{array}\right)$$                    
      $$\operatorname{Cov}(\vec{x}, \vec{y} \cdots \vec{w})=\left(\begin{array}{cccc}\operatorname{Var}(\vec{x}) & \operatorname{Cov}(\vec{y} \vec{x}) & \cdots & \operatorname{Cov}(\vec{w} \vec{x}) \\ \\ \operatorname{Cov}(\vec{x} \vec{y}) & \operatorname{Var}(\vec{y}) & \cdots & \operatorname{Cov}(\vec{w} \vec{y}) \\ \vdots & \vdots & \ddots & \vdots \\ \operatorname{Cov}(\vec{x} \vec{w}) & \operatorname{Cov}\left(\vec{y} \vec{w}\right) & \cdots & \operatorname{Var}(\vec{w})\end{array}\right)$$                    
            - 矩阵对角化
                
                根据优化的条件，我们需要$\operatorname{Cov}(\vec{x}, \vec{y} \cdots \vec{w})$的对角取最大值，其余元素均为0
                
      $$\begin{aligned} \operatorname{Cov}_{y} &=\frac{1}{n} Y Y^{\top} \\ &=\frac{1}{n}(P X)(P X)^{\top} \\ &=P\left(\frac{1}{n} XX^{\top}\right) P^{\top} \\ &=P \operatorname{Cov}_{x} P^{\top} \end{aligned}$$                
                其中，$Y$为$X$在基向量$P$方向上的重映射，探讨$X$与$Y$的协方差矩阵关系
                
                最终，问题变为了寻找一个$P$，使得$P \operatorname{Cov}_{x} P^{\top}$为对角矩阵。
                
                由于${Cov}_{y}$*与${Cov}_{x}$*分别为对称矩阵，均可表示为$Cov = (\lambda_1 \vec{e_1}, \lambda_2 \vec{e_2}, \cdots)$
                
                其中$E=(\vec{e_1},\vec{e_2},\cdots)$，为单位正交特征向量（Eigon Vector）的行矩阵
                
                其中$\Lambda = \left(\begin{array}{llll}\lambda_{1} & & & \\ & \lambda_{1} & & \\ & & \ddots & \\ & & & \lambda_{n}\end{array}\right)$，为特征值（Eigon Value）的对角矩阵
                
                由于$P \operatorname{Cov}_{x} P^{\top} = Cov_y$为对称矩阵，故每一列$A$都有$\lambda_k\vec{e_k}=A\vec{e_k}$
                
                最终，我们可以构造出$P \operatorname{Cov}{x} P^{\top} = Cov_y=\Lambda=E\operatorname{Cov}{x}E^T$
                
                因此$P = E$，最终，我们只需要找出$Cov_y$的特征向量矩阵$E$，我们就得到了基向量矩阵$P$。
                
        - 参考链接
            
            [【机器学习】降维--PCA（非常详细）](https://zhuanlan.zhihu.com/p/77151308)
            
    - 集群分析（K-Mean）
        
        <aside>
        💡 集群分析可以判断大客户行为模式的特点
        
        </aside>
        
        - 具体操作步骤
            
            ```python
            from sklearn.cluster import KMeans
            # 設定集群數
            num_of_cluster = 4
            # 指派集群
            model = KMeans(n_clusters=num_of_cluster, random_state=0)
            model.fit(features)
            pred_class = model.labels_
            print(pred_class)
            ```
            
        - 数学原理分析
            1. 随机集群为$n$个，$A=(a,b,c\cdots)，B，C\cdots$
            2. 计算各个集群的样本均值
                
      $$\bar{A}=\frac{1}{n}\sum_{i=1}^{n}A_i,\ \bar{B}=\frac{1}{n}\sum_{i=1}^{n}B_i,\cdots$$                
            3. 重新计算各样本与样本均值的距离，将样本重新集群到较近的集群中
                
                $\Omega=(a,b,c,\cdots,m,n)$，以样本$a$为例，$L_a=(a-\bar{A}, a-\bar{B},\cdots)$
                
                于是，样本$a$被重新归类到$X$之中，$a-\bar{X}=min(L_a)$
                
            4. 重复归类，直到归类结果不再发生变动
        - 参考链接
            
            [【机器学习】K-means（非常详细）](https://zhuanlan.zhihu.com/p/78798251)
            
- 分类过程
    - 逻辑斯蒂回归
        
        <aside>
        💡 逻辑斯蒂回归是线性回归与逻辑斯蒂函数的叠加模型
        
        </aside>
        
        - 数学原理分析
            
            线性回归是利用一次函数拟合变量关系的回归过程，我们可以对其嵌套逻辑斯蒂函数，将回归过程变为分类过程：
            
      $$\text{logi}(x)=\sigma(\text{reg}(x))=\frac{1}{1+e^{-(m\cdot x + b)}}$$            
            假定数据由二维坐标 $(x_1, x_2)$ 给出，我们可以用它来解决二分类问题：
            
      $$z=w_0+w_1x_1+w_2x_2$$            
            其中，$z$ 代表二维平面中的决策分界线，在一般的回归过程中，如果 $h_w(x) = w_1x_1+w_2x_2+b > 0$，那么其为甲类，反之。但如果我们直接量定 $z$ 大小，采用阶梯函数来分类并不可行，因为阶梯函数不可微，难以通过损失函数优化分类。因此我们使用连续可微的逻辑斯蒂函数将 $h_w(x)$ 映射到了 $A=(0, 1)$ 区间内：
            
      $$p =  \frac{1}{1+e^{-(w^{T} x + b)}}$$            
            如果 $p>0.5$，则为甲类，反之为乙类。
            
            我们采用最大似然方法，使得模型输出的概率最符合给定数据，我们进行如下处理，使得数据尽可能远离决策分界线，即以下两值尽可能大：
            
      $$\begin{aligned} P(Y=1|x) &= p(x) \\  P(Y=0|x) &= 1- p(x) \end{aligned}$$            
            我们有似然函数如下：
            
      $$\begin{aligned} L(w)=&\prod_i[p(x_{i})]^{y_{i}}[1-p(x_{i})]^{1-y_{i}} \\ =&\sum_i[y_{i}\ln p(x_{i})+(1-y_{i})\ln(1-p(x_{i}))] \\ =&\sum_i[y_{i}\ln\frac{p(x_{i})}{1-p(x_{i})}+\ln(1-p(x_{i}))]\end{aligned}$$            
            我们发现 $\ln\frac{p}{1-p}=w^{T} x + b=h_w(x)$，因此：
            
      $$L(w)=\sum[y_{i}(w \cdot x_{i}) - ln(1+e^{w \cdot x_{i}})]$$            
            我们可以进一步计算负平均对数似然损失，转化为损失函数求最小值：
            
      $$\begin{aligned} J(w)&=-\frac{1}{n}\ln L(w) \\&=  -\frac{1}{n}\sum_{i=1}^n[y_i\ln p(x_i)+(1-y_i)\ln(1-p(x_i)]\end{aligned}$$            
    - 决策树（ID3，C4.5，CART）
        
        <aside>
        💡 利用决策树可以推断行为模式的成因
        
        </aside>
        
        - 具体操作步骤
            1. 设定目标变数，为理想的分类结果
                
                ```python
                import numpy as np
                # 設定要分析的類別
                target_class = 1
                # 建立目標變數
                num = len(pred_class)
                data_o = np.zeros(num)
                for i in range(num):
                    if pred_class[i]==target_class:
                        data_o[i] = True
                    else:
                        data_o[i] = False
                print(data_o)
                
                ```
                
            2. 设定说明变数（原始未分类数据），进行决策树分类
                
                ```python
                # 建立說明變數
                data_e = features
                print(data_e)
                ```
                
            3. 生成决策树，使说明变数通过$n=2$次决策达到目标变数
                
                ```python
                from sklearn.tree import DecisionTreeClassifier
                from sklearn.tree import export_graphviz
                
                # 建立決策樹的模型
                clf = DecisionTreeClassifier(max_depth=2)
                clf = clf.fit(data_e, data_o)
                ```
                
            4. 绘制决策树，输出决策的结果
                
                ```python
                from dtreeviz.trees import dtreeviz
                
                # 篩選出index
                x_0 = df_info.resample('M').count()
                x_0 = x_0.drop(x_0.columns.values,axis=1)
                time_index = x_0.index
                print(time_index)
                
                # 繪製決策樹
                viz = dtreeviz(
                    clf,
                    data_e,
                    data_o,
                    target_name='Class',
                    feature_names=time_index,
                    class_names=['False','True'],
                )
                viz
                ```
                
            5. 可视化分类结果
                
                ```python
                from sklearn.decomposition import PCA
                import numpy as np
                import matplotlib.pyplot as plt
                import matplotlib.patches as pat
                
                # 進行分類
                pred_tree = clf.predict(data_e)
                
                # 執行主成分分析
                pca = PCA()
                pca.fit(features)
                # 將特徵向量轉換成主成分
                transformed = pca.fit_transform(features)
                # 可視化
                plt.figure(figsize=(12, 8))
                plt.scatter(transformed[:,0],transformed[:,1],c=pred_class)
                for i in range(len(transformed)):
                    if pred_tree[i]==1:
                        if pred_class[i]==1:
                            temp_color = "k"
                            temp_lw = 1.0
                        else:
                            temp_color = "b"
                            temp_lw = 3.0
                        circle = pat.Circle(xy=(transformed[i,0],transformed[i,1]), 
                				radius=1.0, ec=temp_color ,fill=False, linewidth = temp_lw)
                        plt.axes().add_artist(circle)
                    else:
                        if pred_class[i]==1:
                            temp_color = "r"
                            temp_lw = 3.0
                            circle = pat.Circle(xy=(transformed[i,0],transformed[i,1]), 
                						radius=1.0, ec=temp_color ,fill=False, linewidth = temp_lw)
                            plt.axes().add_artist(circle)
                    text = str(i) + "(" + str(pred_class[i]) + ")"
                    plt.text(transformed[i,0],transformed[i,1],text)
                plt.show()
                %matplotlib inline
                ```
                
        - 数学原理分析
            
            <aside>
            💡 构建决策树的核心思想为寻找使熵下降最快的一种决策过程组合
            
            </aside>
            
            - 总体思路
                - 初始化特征集合，准备学习已分类数据集合（带有特征标签和分类结果标签）
                - 对每个待选特征计算该算法的判断指标，最大的特征作为当前决策节点
                - 更新数据集合，从特征集合中划去已使用的特征
                - 剪枝，防止过拟合，减小计算量，增大分类速度
            - ID3决策树生成
                - 判断指标
                    - 信息熵
                        
                        ID3决策树是一种以决策过程的熵值作为判断依据而生成的决策树。
                        
                        假设决策树为二叉树结构，经过$n$个决策过程，我们能得到分类后的结果。
                        
                        那么对于某个决策过程（二叉树节点），我们有决策结果（分类结果）：
                        
      $$R=\{x_1, x_2, x_3\cdots\}$$                        
                        关于这个决策过程，结果的平均信息熵（Etropy）定义为$E$：
                        
      $$E=-\sum_{R} P(x_i)\cdot log_2P(x_i)$$                        
                        如果决策树为二叉树，对每一个特征只有两种分类结果$R=\{x_1, x_2\}$
                        
                        对于这个决策过程，有平均信息熵：
                        
      $$E=-P(x_1)\cdot log_2P(x_1)-P(x_2)\cdot log_2P(x_2)$$                        
      $$\left\{ \begin{array} {l} E_0=-\frac{1}{2}\cdot log_2\frac{1}{2}-\frac{1}{2}\cdot log_2\frac{1}{2}=1 & \\ E_1=-1\cdot0-0\cdot 1=0 \end{array} \right.$$                        
                        我们发现良好的决策（低随机性，高度干涉的分类方式）带来的熵更低，不良的决策（随机较高，$p=0.5$）带来的熵更高，良好的决策使系统熵值快速下降。
                        
                    - 信息增益
                        
                        定义决策过程使熵下降大小为该决策的信息增益（Information Gain）为$G$：
                        
      $$G_{p=n}=E_{p=n-1}-E_{p=n}$$                        
                        $E_{p=n-1}$为经验熵，分裂之前的决策错误率：
                        
      $$E_{p=n-1}=H(D)=-\sum_{R} P(x_i)\cdot log_2P(x_i)$$                        
                        $E_{p=n}$为条件熵，选定某个特征$A$作为决策过程后的熵值：
                        
      $$\begin{aligned}E_{p=n} = & H(D|A) \\= & \sum_{a\in R_A} H(D|A=a)\\= &  -\sum_{R} P(x_j)\cdot log_2P(x_j)\end{aligned}$$                        
                        其中，$x_j$为本次决策过程的某个结果，$x_i$为上次决策过程的某个结果。
                        
                        在ID3算法中，我们判断信息增益$G$的大小，排布决策过程，判断父节点关系，得到信息熵下降最快的决策过程组合。
                        
            - C4.5决策树生成
                - 判断指标
                    
                    <aside>
                    💡 过拟合一般表现为过于偏向那些取值较多的特征进行分类
                    
                    </aside>
                    
                    C4.5克服了ID3的过拟合问题（Overfitting），利用信息增益率（Information Gain Ratio）作为判断依据生成决策树：
                    
      $$Gr=\frac{Gain(A)}{Split(A)}=\frac{G_{p=n}}{E_{p=n}}$$                    
                    其中，分母代表决策过程结果的信息。
                    
                    信息增益除以结果信息，可以有效遏制其过分选择结果特别多的决策过程。
                    
                - 剪枝策略
                    
                    <aside>
                    💡 为了防止过拟合带来的影响，C4.5模型引入了剪枝策略
                    
                    </aside>
                    
                    - 预剪枝
                        1. 分裂前样本$d\in D$均小于某预先设定好的阈值$\alpha$，不分裂
                        2. 所有的特征$a\in A$均已经分裂过，不分裂
                        3. 分裂后，分类的准确性降低，不分裂
                    - 后剪枝
                        
                        由低位到高位递归，对除叶子节点（End Point）以外节点，判断其不分裂是否更优（用一个最优叶子节点代替子树），以样本的错误分类数量为依据。
                        
                        ![graph(1).png](%E8%A1%8C%E4%B8%BA%E5%88%86%E6%9E%90%203aa5b757281d4f529f9103b0a4e7d243/graph(1).png)
                        
                        其中，$C_n$代表决策特征，$R_n$代表归类的结果，比如最终结果有三种类别，那么$R_n\in \{x_1,x_2,x_3\}$。
                        
            - CART决策树生成
                - 判断指标
                    
                    <aside>
                    💡 基尼不纯度避免了熵的对数运算，速度提升
                    
                    </aside>
                    
                    CART利用基尼不纯度作为判断依据生成二叉决策树：
                    
      $$Gini = \sum_{i=1}^{k}{}p_i\cdot(1-p_i)=1-\sum_{i=1}^{k}p_i^{2}$$                    
                    其中，$p_i$为决策结果的可能性。
                    
                    基尼指数表示了从训练集中随机抽取两个样本，类别不一致的概率。
                    
                    当 $p_i=p_j=\cdots=\frac{1}{k}$ 时，基尼不纯度最大，样本纯度低，特征较好。
                    
                    当 $p_i=1,p_j=p_k=\cdots=0$ 时，基尼不纯度最小，样本纯度高。
                    
                    特别地，基尼不纯度近似地为信息熵的一阶泰勒展开：
                    
      $$E=-\sum_{k=1}^{K} p_{k} \ln p_{k}\approx \sum_{k=1}^{K} p_{k}\left(1-p_{k}\right)=Gini$$                    
                    CARD决策树生成没有停止条件，会一直生长。
                    
                - 剪枝策略
                    
                    CART算法使用代价复杂度剪枝方法：
                    
                    节点越多，误差越小，但有的节点对误差减少的贡献率大，有的贡献小。我们找出那些使用较多子节点，但贡献小的子树，把它剪裁掉。
                    
                    于是小树大贡献保留，大树小贡献剪枝，因此我们需要一种同时衡量一棵树对误差减小贡献与树的规模的方法：
                    
      $$C_\alpha(T)=C(T)+\alpha|T|$$                    
                    其中，$C(T)$为树$T$使分类误差减小的值（基尼指数），$|T|$为树$T$的节点数量，$\alpha$为权衡树的规模与误差减小贡献的参数。
                    
                    具体而言，对于每一个子树$T_t$而言，剪枝之前都有：
                    
      $$C_\alpha(T_t)=C(T_t)+\alpha|T_t|$$                    
                    其中，$t$代表子树$T_t$的根节点，$C(T_t)$为树$T$使分类误差减小的值（基尼指数）。
                    
                    而剪枝之后都有：
                    
      $$C_\alpha(t)=C(t)+\alpha\cdot1$$                    
                    因此，我们在剪枝阶段，首先有CART算法生成的$T_0$，有$\alpha=+\infty$，对于$T_0$的每一个子节点$t$，我们都（由下而上地）计算指标$g(t)$：
                    
      $$g(t) = \frac{C(t)-C(T_t)}{|T_t|-1}  \\$$                    
      $$\alpha=min(\alpha, g(t))$$                    
                    其中，$T_t$为以$t$为根节点的子树。
                    
                    其次，（由上而下地）计算所有$t$节点的$g(t)$，如果$g(t)=a$，那么$t$对应的$T_t$为$T_0$的最差子树，将$T_t$从$T_0$中剪除。$t$成为了叶子节点，将剪除前$T_t$的最多的分类结果作为叶子节点$t$的分类结果，得到剪除后的CART生成树$T_1$。
                    
                    接着，由$T_0$进行了一次剪除，我们得到了$T_1$，对$T_1$重复上述操作，递归直到树只剩单个节点。最终我们得到了$\{T_0,T_1\cdots T_n\}$，使用用来测试的另一份人工标记数据集，对$\{T_0,T_1\cdots T_n\}$内所有树逐一测试分类准确度，选择最优树$T_k$。
                    
        - 参考链接
            
            [【机器学习】决策树（上）--ID3、C4.5、CART（非常详细）](https://zhuanlan.zhihu.com/p/85731206)
            
            [CART决策树剪枝--机器学习](https://zhuanlan.zhihu.com/p/521202734)
            
    - 随机森林（Random Forests）
        
        <aside>
        💡 决策森林减少了数据选取的特异性，特征选取的特异性对分类的影响
        
        </aside>
        
        - 具体操作步骤
            
            生成样本的随机森林，与其对应的混淆矩阵
            
            ```python
            from sklearn.model_selection import train_test_split
            from sklearn.ensemble import RandomForestClassifier
            from sklearn.metrics import confusion_matrix
            
            # 將資料集分割成訓練資料與評估資料
            x_train, x_test, y_train, y_test = train_test_split(features,data_o)
            
            # 利用訓練資料建立模型
            model = RandomForestClassifier(bootstrap=True, n_estimators=10, max_depth=None, random_state=1)
            clf = model.fit(x_train, y_train)
            
            # 利用評估資料進行評估
            # 計算分數
            score = clf.score(x_test, y_test)
            print("分數:",score)
            
            # 產生混淆矩陣
            pred_tree = clf.predict(x_test)
            cm = confusion_matrix(y_test, pred_tree)
            print("混淆矩陣")
            print(cm)
            ```
            
        - 数学原理分析
            
            将 $n$ 个样本有放回地从 $N$ 个样本中抽出，将 $m$ 个特征从 $M$ 个样本中抽出，以 $n$ 个样本，$m$ 个特征组成一棵决策树。
            
            ![Web 1920 – 1.png](%E8%A1%8C%E4%B8%BA%E5%88%86%E6%9E%90%203aa5b757281d4f529f9103b0a4e7d243/Web_1920__1.png)
            
            重复上述过程，得到数棵决策树，组成决策森林。最终的分类结果由数棵决策树的结果同时决定，最终我们得到投票决定的分类结果。
            
        - 参考链接
            
            [随机森林详解（从决策树理解随机森林）](https://zhuanlan.zhihu.com/p/471494060)
            
    - 支援向量机（SVM）
        
        <aside>
        💡 支援向量机利用维度面将特征向量切成两部分，实现二分类操作
        
        </aside>
        
        - 具体操作步骤
            
            利用支援向量机进行样本二分类
            
            ```python
            from sklearn.model_selection import train_test_split
            from sklearn.svm import SVC
            from sklearn.metrics import confusion_matrix
            
            # 將資料集分割成訓練資料與評估資料
            x_train, x_test, y_train, y_test = train_test_split(features,data_o)
            
            # 利用訓練資料建立模型
            model = SVC(kernel='rbf')
            clf = model.fit(x_train, y_train)
            
            # 利用評估資料進行評估
            # 計算分數
            score = clf.score(x_test, y_test)
            print("分數:",score)
            
            # 產生混淆矩陣
            pred_tree = clf.predict(x_test)
            cm = confusion_matrix(y_test, pred_tree)
            print("混淆矩陣")
            print(cm)
            ```
            
        - 数学原理分析
            
            在空间中有 $V=(\vec{x_1}, \vec{x_2}\cdots\vec{x_n})$，利用一（超）平面将向量切分为两部分，实现二分类，两类中距离超平面最近的向量被称为支持向量。
            
            - 低维线性可分的SVM
                
                对于线性可分的数据而言，我们可以找到无数个超平面 $X^T\cdot\vec{w}+b=0$，将  $V$ 分为两部分，其中 $X^T$ 为面向量，$\vec{w}$ 为平面法向量，$b$ 为截距。
                
                ![v2-197913c461c1953c30b804b4a7eddfcc_1440w.jpg](%E8%A1%8C%E4%B8%BA%E5%88%86%E6%9E%90%203aa5b757281d4f529f9103b0a4e7d243/v2-197913c461c1953c30b804b4a7eddfcc_1440w.jpg)
                
                图中虚线穿过的向量为支持向量 $\vec{x_s}$，其符合 $\vec{x_s}\cdot\vec{w}+b=\pm 1$
                
                支持向量与超平面的距离称为间隔（Margin），记为 $\rho$：
                
      $$\rho=\frac{2}{\|\vec{w}\|}$$                
                其中，$\|\vec{w}\|$ 为高维向量的模长。
                
                我们利用间隔大小判断超平面选取的优劣。间隔越大，则分类容忍度越高，间隔越小，越容易发生过拟合。由此看来，对于超平面 $X^T\cdot\vec{w}+b=0$，优化的手段为找到 $\rho$ 的最大值：
                
      $$max_{\vec{w}, b}(\rho)\iff min_{\vec{w}, b}(\|\vec{w}\|)\iff min_{\vec{w}, b}(\frac{1}{2}\cdot\|\vec{w}\|^2)=J(w)_{min}$$                
                对于给定超平面 $\vec{x_s}\cdot\vec{w}+b=\pm 1$ ，定义超平面关于样本点 $\left( x_i,y_i \right)$ 的几何间隔：
                
      $$\gamma _i=y_i\left( \frac{\vec{w}}{\lVert \vec{w} \rVert}\cdot \vec{x}_{i}+\frac{b}{\lVert \vec{w} \rVert} \right)$$                
                在所有样本点 $\left( x_i,y_i \right)_{ i=1,2,\cdots ,N}$  中，几何间隔最小值为：
                
      $$\gamma_{min} =\underset{i=1,2,\cdots ,N}{\min}\gamma _i=\rho$$                
                除支持向量外，所有向量关于超平面的几何间隔都大于 $\gamma_{min}$：
                
      $$y_i\left( \frac{\vec{w}}{\lVert \vec{w} \rVert}\cdot \vec{x}_{i}+\frac{b}{\lVert \vec{w} \rVert} \right) \ge \gamma_{min}=\rho \ ,i=1,2,...,N$$                
                化简得 $\rho=J(w)_{min}=\frac{1}{2}\cdot\|\vec{w}\|^2$ 的约束条件：
                
      $$y_i\left( \frac{\vec{w}}{\lVert \vec{w} \rVert\cdot \rho}\cdot \vec{x}_{i}+\frac{b}{\lVert \vec{w} \rVert\cdot \rho} \right) \ge 1 \ ,i=1,2,...,N$$                
                这里是一个凸优化问题，利用带有 $KKT$ 条件的拉格朗日乘子法求解 $\rho$ 的最大值，我们将多个不等式约束下的$J(w)_{min}$转化为无约束的拉格朗日函数：
                
      $$L(\vec{w},b,\lambda_i,p_i)=\frac{1}{2}\cdot\|\vec{w}\|^2-\sum_{i=1}^{N}\lambda_i (y_i\left(W\cdot \vec{x}_{i}+B\right)-1-{p_i}^2)$$                
                由约束条件得，可行域为$A\cup B$：
                
                其中，$\lambda_i$ 为每个约束条件的拉格朗日乘子，$\lambda_i\ge0$，${p_i}^2$ 将不等式调平为等式，为了方便表示，我们写作 $W=\frac{\vec{w}}{\lVert \vec{w} \rVert\cdot \rho}$，$B=\frac{b}{\lVert \vec{w} \rVert\cdot \rho}$
                
                由约束条件，样本点 $(x_i,y_i)$ 的可行域为：
                
      $$A=\{(x_i,y_i)\ | \ y_i\left( W\cdot \vec{x}_{i}+B \right) > 1\}$$                
      $$B=\{(x_i,y_i)\ | \ y_i\left( W\cdot \vec{x}_{i}+B \right) = 1\}$$                
                对 $L(\vec{w},b,\lambda,p_i)$ 左右分别对 $\vec{w},b,\lambda,p_i$ 求偏导数，使其分别等于$0$
                
                我们得到如下四个 $KKT$ 条件：
                
                不等式约束的拉格朗日函数规定，如果样本点 $(x_i,y_i)\in A$，那么 $\lambda_i=0$，如果样本点 $(x_i,y_i)\in B$，那么 $\lambda_i\neq0$，如果样本点脱离可行域，不在讨论范围。
                
                1. $\vec{w}-\sum_{i=1}^{N}\lambda_i\cdot y_i \cdot x_i=0$
                2. $\sum_{i=1}^{N}\lambda_i\cdot y_i=0$
                3.  $y_i\left(W\cdot \vec{x}_{i}+B\right)-1-{p_i}^2=0$
                4. $2 \lambda_i  p_i=0$
                
                由3，4得到：$\lambda_i[y_i\left(W\cdot \vec{x}_{i}+B\right)-1]=0$
                
                如果样本点 $(x_i,y_i)\in A$，那么 $\lambda_i=0$ 必然成立，如果样本点$(x_i,y_i)\in B$，那么$\lambda_i\neq0$ 可以成立。由于 $\lambda_i$ 为两梯度向量的正向长度比，因此最后$\lambda_i \geq 0$
                
                利用如上四个原 $KKT$ 条件，以及两个衍生条件，我们可以求解最优超平面。
                
            - 核技巧（低维线性不可分的SVM）
                
                <aside>
                💡 将低维不易二分类数据映射到高维空间后分类时，简化高维点积计算方法
                
                </aside>
                
                当对偶问题中 $\vec{x}_{i}\cdot \vec{x}_{j}$ 不可解时，我们利用升维函数 $T(\vec{x})$，将 $\vec{x}$ 映射到高维空间，并计算 $T(\vec{x}_{i})\cdot T(\vec{x}_{j})=\sum_{m=1}^N x_{im}x_{jm}$
                
                为了简化以上高维点积，我们尝试找到核函数 $T(\vec{x}_{i})\cdot T(\vec{x}_{j})=K(\vec{x}_{i}, \vec{x}_{j})$
                
                如果我们能找到核函数 $K(\vec{x}_{i}, \vec{x}_{j})$，那么我们就可以直接跳过升维步骤，直接计算两向量在高维的点积结果：
                
      $$K_{c,d}(\vec{x}_{i}, \vec{x}_{j})=(c+\vec{x}_{i}\cdot\vec{x}_{j})^d$$                
                其中，不同 $c,d$ 的组合能控制转化后的维度和空间相似度，影响超平面的选取。
                
                更高级地，如果我们想要寻找 $\vec{x}_{i}\cdot \vec{x}_{j}$ 在无限高维度上的点积结果，我们使用需要高斯核函数（RBF）：
                
      $$K_\gamma(\vec{x}_{i}, \vec{x}_{j})=e^{-\gamma\|\vec{x}_{i}-\vec{x}_{j}\|^2}$$                
                其中，$\|\vec{x}_{i}-\vec{x}_{j}\|^2$ 为两点间距离的平方，$K_\gamma$ 反映了两向量相似度的大小，$\gamma$  反映了核函数对相似度的判定标准。
                
                两向量点距离越远，相似度越接近 $0$，两向量点距离越近，相似度越接近 $1$，
                
                $\gamma$ 较小，相似度变动迟钝，$\gamma$ 较大，相似度变动灵敏。
                
                关于为何高斯核函数可以计算无限维度下向量相似度的结果，我们转化核函数为：
                
      $$K_\gamma(\vec{x}_{i}, \vec{x}_{j})=e^{-\gamma\|\vec{x}_{i}-\vec{x}_{j}\|^2}=C\cdot e^{\vec{x_1}\cdot\vec{x_1}} \\ C=e^{-\gamma(\|x_i^2\|+\|x_j^2\|)}$$                
                接着对 $e^{\vec{x_1},\vec{x_1}}$泰勒展开，我们得到：
                
      $$K_\gamma(\vec{x}_{i}, \vec{x}_{j})=C\cdot e^{\vec{x_1}\cdot\vec{x_1}}=C\cdot\sum_{n=0}^\infty \frac{\vec{x}_{i}\cdot \vec{{x}_{j}}^n}{n!}=C\cdot\sum_{n=0}^\infty \frac{K_{0,n}(\vec{x}_{i}, \vec{x}_{j})}{n!}$$                
                我们由此发现高斯核函数为所有维度从低到高的调参组合，蕴含了所有维度下向量点乘的信息，由此我们将这个蕴含所有维度点积信息的值称为无限维度下向量相似度。
                
            - 软间隔
                
                <aside>
                💡 当出现破坏间隔的点时，采用量化误差衡量超平面的选取
                
                </aside>
                
                当数据点分布到间隔内部，不符合约束条件时，我们计算它与正（负）超平面 $\vec{x}\cdot\vec{w}+b=\pm 1$ 的距离，将其作为一个可量化的误差：
                
      $$\epsilon_i=max(0, 1-y_i\cdot(\vec{x}\cdot W+B))$$                
                我们向原超平面硬间隔求解中引入损失值的概念，得到问题：
                
      $$J(\vec{w})_{min}= \ min_{\vec{w}, b}\left(\frac{1}{2}\cdot\|\vec{w}\|^2+\sum_{i=1}^N  \epsilon_i \right)\\  \ s.t. \ \  g_i(\vec{w}, b)= y_i\left( W\cdot \vec{x}_{i}+B \right) > 1$$                
                其中，$\epsilon_i= \max(0, 1-y_i\cdot(\vec{x}\cdot W+B))$ 为铰链损失函数。由于$\|\vec{w}\|$ 越大，间隔越小，越不能出错，$\epsilon_i$ 越小，反之亦然。因此 $\|\vec{w}\|$ 与 $\epsilon_i$ 相互制约，达到平衡。
                
                $J(\vec{w})$ 中的 $\epsilon_i$ 作为自变量出现时，我们将 $\epsilon_i$ 的解析式转化为两个约束条件：
                
                1. $\epsilon_i \geq1-y_i\cdot(\vec{x}\cdot W+B)$
                2. $\epsilon_i \geq 0$
                
                以及先前的几个约束条件：
                
                1. $\lambda_i \geq 0$
                2. $\lambda_i[y_i\left(W\cdot \vec{x}_{i}+B\right)-1+\epsilon_i]=0$
                
                在实际操作中，可以在损失值前乘 $C$，控制损失值对超平面选取的影响。最后利用如上约束条件，再利用对偶性解出软间隔下超平面，得到问题：
                
      $$q(\lambda_i)_{max}=\left[\sum_{i=1}^{N} \lambda_{i}-\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \lambda_{i} \lambda _j y_{i} y_{j} (\vec{x}_{i}\cdot \vec{x}_{j})\right]_{max}\\ s.t. \ \ \sum_{i=1}^N \lambda_iy_i=0,0 \leq \lambda_i \leq C$$                
            - 拉格朗日对偶优化求解
                
                <aside>
                💡 利用优化问题强对偶性，构造原函数的下界函数 $q(\lambda_i)_{max}$，侧面求得结果
                
                </aside>
                
                我们求解超平面的的原始方法为：
                
      $$J(\vec{w})_{min}=min_{\vec{w}, b}(\frac{1}{2}\cdot\|\vec{w}\|^2) \\ s.t. \ \ g_i(\vec{w}, b)=y_i\left( W\cdot \vec{x}_{i}+B \right) > 1$$                
                其中，超平面的最优解在 $\vec{w}^*,b^*$ 处取得。
                
                我们定义新方程，$q(\lambda_i):=L(\vec{w},b,\lambda_i)_{min}\leq J(\vec{w^*})-\sum_{i=1}^{N}\lambda_i g_i(\vec{w^*}, b^*)$
                
                因为 $\lambda_i\geq0$， $g_i(\vec{w}, b) \geq 0$，我们可以得到不等式：
                
      $$q(\lambda_i)\leq J(\vec{w^*})-\sum_{i=1}^{N}\lambda_i g_i(\vec{w^*}, b^*)\leq J(\vec{w^*})\leq J(\vec{w})$$                
                因此我们可以看出 $q(\lambda_i)$ 是 $J(\vec{w})$ 的下界，我们还有不等式：
                
      $$q(\lambda_i)\leq q(\lambda^*_i)\leq J(\vec{w^*})\leq J(\vec{w})$$                
                如果 $q(\lambda^*_i)< J(\vec{w^*})$，则为弱对偶问题，但如果 $q(\lambda^*_i)= J(\vec{w^*})$，为强对偶问题，我们可以求解对偶问题，此时原问题与对偶问题会同时解决，我们寻找下界 $q(\lambda_i)$ 的最大值 $q(\lambda^*_i)$ 时，就同时找到了 $J(\vec{w})$ 的最小值 $J(\vec{w^*})$，即 $J(\vec{w})_{min}$。
                
                因为原问题满足$Slater$条件，因此为强对偶问题，我们构造出原问题的对偶问题：
                
      $$q(\lambda_i)_{max}=\left[L(\vec{w},b,\lambda_i)_{min} \right]_{max}\\ s.t. \ \ \lambda_i\geq0$$                
                我们先求解 $L(\vec{w},b,\lambda_i)_{min}$：
                
      $$L(\vec{w},b,\lambda_i)_{min}=\frac{\| \vec{w}\|^2}{2}-\sum_{i=1}^{N} \lambda_{i}\left(y_{i}\left(\vec{w} \cdot \vec{x_{i}}+b\right)-1\right)$$                
                化简过程中，我们套用如下 $KKT$ 条件：
                
                1.  $\vec{w}-\sum_{i=1}^{N}\lambda_i\cdot y_i \cdot x_i=0$
                2. $\sum_{i=1}^{N}\lambda_i\cdot y_i=0$
                
                代入原式中，我们得到：
                
      $$\begin{aligned}L(\vec{w},b,\lambda_i)_{min}=\ & \frac{1}{2}\left(\sum_{i=1}^{N} \lambda_{i} y_{i} \vec{x}_{i}\right) \left(\sum_{i=1}^{N} \lambda_{j} y_{j} \vec{x}_{j}\right) -\\ & \sum_{i=1}^{N} x_{i} \left(y_{i} \left(\left(\sum_{j=1}^{N} \lambda_jy_{j}\vec{x}_{j}\right) \vec{x}_{i}+b\right)-1\right)\\=\ & \sum_{i=1}^{N} \lambda_{i}-\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \lambda_{i} \lambda _j y_{i} y_{j} \vec{x}_{i} \vec{x}_{j}\end{aligned}$$                
                于是原问题的对偶问题转换成：
                
      $$q(\lambda_i)_{max}=\left[\sum_{i=1}^{N} \lambda_{i}-\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \lambda_{i} \lambda _j y_{i} y_{j} (\vec{x}_{i}\cdot \vec{x}_{j})\right]_{max}\\ s.t. \ \ \lambda_i\geq0$$                
                我们解出取极值时 $\lambda_i$ 的值，利用 $KKT$ 条件，求出$\vec{w}=\sum_{i=1}^{N}\lambda_i\cdot y_i \cdot x_i$
                
                而且对于所有向量而言，只有支持向量 $\vec{x_i}\in B$，$\lambda_i \neq 0$，因此只有支持向量参与到对偶问题的计算之中。
                
        - 参考链接
            
            [看了这篇文章你还不懂SVM你就来打我](https://zhuanlan.zhihu.com/p/49331510)
            
            [支持向量机（SVM）--原理篇](https://zhuanlan.zhihu.com/p/31886934)
            
            [Karush-Kuhn-Tucker (KKT)条件](https://zhuanlan.zhihu.com/p/38163970)
            
    - 高斯混合模型（GMM）
        
        <aside>
        💡 高斯混合模型是分类模型，处理同一集合下的数据包含多种不同分布的情况
        
        </aside>
        
        - 数学原理分析
            
            高斯混合模型的本质是一个混合概率分布：
            
      $$P(x)=\sum_{k=1}^KW_k\cdot \phi(x\mid\mu_k,Cov_k)$$            
            其中，$k$ 为 GMM 模型中成分个数，$\phi(x)$ 为高斯分布的概率密度函数，分布的均值为 $\mu_k$，其协方差矩阵为 ${Cov}_k$，$W_k$ 为各成分权重。
            
            我们想要将数据分为 $n$ 类，即簇数 $K=n$，计算 $\vec{x}=(\mu_k,\theta_k,W_k)，k\in[1,K]$  
            
            最终从全体数据的概率分布 $P(x)$ 中摘出 $n$ 个正态分布，分别表示 $n$ 类数据。
            
            对于参数的求解，我们使用 EM 算法，即最大似然算法：
            
            我们写出概率函数 $P(x)$ 的似然函数：
            
      $$L(\theta)=\prod_{i=1}^{n}[\sum_{j=1}^{n} W_j\phi(x|\mu_j,Cov_j)]$$            
            简化协方差矩阵 $Cov_k$ 为 $\sigma_k$，等式两侧取对数：
            
      $$l(\theta)=\ln(L(\theta))=\sum_{i=1}^{n}\ln[\sum_{j=1}^{n} W_j\phi(x|\mu_j,\sigma_j)]$$            
            等式两侧求偏导数：
            
      $$\frac{\partial l}{\partial \mu}=$$            
      $$\frac{\partial l}{\partial \sigma}=$$            
            高斯混合模型不只是一个分类问题的解决方案，也是一个生成模型。我们在将数据分类为 $n$ 个高斯分布之后，也变相构造了一个叠加的总概率分布。利用这个叠加的总概率分布，我们也可以用它来生成新数据。
            
    - 隐马尔可夫模型（HMM）
        - 数学原理分析
    - 混淆矩阵（Confusion Matrix）
        - 具体操作步骤
            
            生成决策树的混淆矩阵
            
            ```python
            from sklearn.metrics import confusion_matrix
            cm = confusion_matrix(data_o, pred_tree)
            print(cm)
            ```
            
        - 数学原理分析
            
            对于类别分析，我们有预测的类别与实际的类别，利用预测类别和实际类别，构建混淆矩阵，判断分类的精确程度，以如下数据为例：
            
            |  | 预测为 A 类 | 预测为 B 类 | 预测为 C 类 | 预测为 D 类 |
            | --- | --- | --- | --- | --- |
            | 实际为 A 类 | 12 | 4 | 3 | 6 |
            | 实际为 B 类 | 3 | 10 | 5 | 3 |
            | 实际为 C 类 | 4 | 3 | 8 | 0 |
            | 实际为 D 类 | 5 | 2 | 4 | 11 |
            
      $$Con = \left(\begin{array}{cccc}12 & 4 & 3 & 6 \\ 3 & 10 & 5 & 3 \\ 4 & 3 & 8 & 0 \\ 5 & 2 & 4 & 11\end{array}\right)$$            
            其中，混淆矩阵正对角线值越大，其他值越小，预测效果约完美。
            
            即准确性（Accuracy）越高，整体预测效果越好：
            
      $$A=\frac{\sum_{i=1}^{N}Con_{ii}}{\sum_{i=1}^{N}\sum_{j=1}^{N}{Con_{ij}}}$$            
            除此之外，我们有两个对应指标来更加细节地衡量预测效果：
            
            1. 精确度（Precision）
                
                精确度是对于我们的预测结果而言的，有多少预测正确了。
                
                即对于某一个类别 $E$ 而言，预测为 $E$ 类的样本中有多少实际属于 $E$ 类：
                
      $$P_I=\frac{Con_{ii}}{\sum_{n=1}^{N}{Con_{ni}}}$$                
                其中，假定混淆矩阵中 $I$ 类相关元素处于第 $i$ 行列，混淆矩阵的大小为 $N\times N$。
                
            2. 召回率/灵敏度（Recall）
                
                召回率是对于原本带标签的数据而言的，有多少被预测正确。
                
                即对所有实际为 $E$ 类别的样本而言，有多少正确的预测为了 $E$ 类：
                
      $$R_I=\frac{Con_{ii}}{\sum_{n=1}^{N}{Con_{in}}}$$                
            3. 同时考虑两个指标，我们有 $F1$ 分数如下：
                
      $$F_1=\frac{1}{\frac{1}{P_I}+\frac{1}{R_I}}$$                
                由此可见，此调和平均数越高，预测效果越好。由于调和平均的特性，$F1$ 分数将更接近两种指标中相对较小的那一个，而非单纯的二者平均值。
                
    - 训练资料分割与预测能力评估
        - 具体操作步骤
            1. 将资料分裂成训练资料与评估资料
                
                ```python
                from sklearn.model_selection import train_test_split
                x_train, x_test, y_train, y_test = train_test_split(features,data_o)
                ```
                
            2. 利用训练资料建构模型，此处为决策树
                
                ```python
                from sklearn.tree import DecisionTreeClassifier, export_graphviz
                clf = DecisionTreeClassifier(max_depth=2)
                clf = clf.fit(x_train, y_train)
                ```
                
            3. 评估精确度
                
                ```python
                from sklearn.metrics import confusion_matrix
                
                # 計算分數
                score = clf.score(x_test, y_test)
                print("分數:",score)
                
                # 產生混淆矩陣
                pred_tree = clf.predict(x_test)
                cm = confusion_matrix(y_test, pred_tree)
                print("混淆矩陣")
                print(cm)
                ```
                
- 回归过程
    
    <aside>
    💡 一般来说，分类的输出是离散的，而回归的输出是连续的
    
    </aside>
    
    - 多元变量线性回归（MLR）
        - 具体操作步骤
            
            ```r
            # 销售渠道与销量的线性相关，marketing 为四列的表格
            model <- lm(sales ~ youtube + facebook + newspaper, data = marketing)
            summary(model)
            ```
            
        - 数学原理分析
            
            我们有 $n$ 个可控变量（Explanatory Variables）$x_1,x_2,\cdots,x_n$，一个因变量 （Dependent Variable）$y$，假定可控变量与因变量间符合一次线性规律，我们有：
            
      $$y_i=c_0+c_1\cdot x_{1_i}+c_2\cdot x_{2_i}+\cdots+c_n\cdot x_{n_i}$$            
            - 概率论诠释
                
                对于二元线性回归，我们想要得到 $y=b_0+b_1x$，将实际数据 $y_i$ 与估计数据 $b_0+b_1x_1$ 纵向相减，平方求和，得到二乘法：
                
      $$L(b_0,b_1)=\sum_{i=1}^n[y_i-(b_0+b_1x_i)]^2$$                
                求解最小二乘法，得到 $b_0$ 与 $b_1$ 的最优参量 $\beta_0$ 与 $\beta_1$：
                
      $$\beta_0,\beta_1=\argmax_{b_0,b_1}L(b_0,b_1)$$                
                通过对 $L(b_0,b_1)$ 关于  $b_0$ 与 $b_1$ 求偏导，得到最优参量：
                
      $$\begin{aligned}&\hat \beta_1=\frac{\sum_{i=1}^n(x_i-\bar x)(y_i-\bar y)}{\sum_{i=1}^n(x_i-\bar x)^2}\\&\hat \beta_0=\bar y-\hat \beta_1\cdot \bar x\end{aligned}$$                
            - 线性代数诠释
                
                对于任一线性系统（Linear System），我们有：
                
      $$A_{m\times n}\cdot \vec x=\vec y$$                
                其中，$\vec x\in R^n$，$\vec y\in R^m$ ，$A_{m\times n}$ 为线性变换的基本矩阵（Elementary Matrix）
                
                当线性系统无解时，我们寻找近似解 $A_{m\times n}\cdot \vec s\approx\vec b$，本质上寻找一个 $\vec s$，使得两向量 $A\vec s$ 与 $\vec b$ 距离最近：
                
      $$l=\min_{\vec s}\Vert A\vec s-\vec b \Vert$$                
                相当于寻找两向量所有元之间差的平方和：
                
      $$l=\min_{\vec s}\sum_{i=1}^m [(A \vec s)_i-\vec b_i]^2$$                
                我们计算 $A^TA\vec s=A^T\vec b$，一定能得到解 $\vec s$ 作为最小二乘的近似解。
                
                - 对此的证明，我们从正交投影的角度出发
                    
                    因为 $A\vec x\in\text{Col(A)}$，在 $\text{Col(A)}$ 内与 $\vec b$ 距离最近向量为 $A\vec s=\text{Proj}_{\text{Col(A)}}\vec b$，具体来说， 我们有：
                    
      $$\vec b-\hat b\in \text{Col}(A)^\perp=\text{Nul}(A^T)$$                    
                    其中，$\hat b=\text{Proj}_{\text{Col(A)}}\vec b$，作为 $\vec b$ 在 $\text{Col}(A)$ 上的正交投影向量。
                    
                    由上可知 $A^T\cdot(\vec b-\hat b)=0$，因此 $\vec b=\hat b$，下面从两个方向证明原式。
                    
                    一方面，我们设 $\vec s\in R^n$ 为最小二乘结果，那么有：
                    
      $$A\vec s=\hat b=\vec b \Rightarrow A^TA\vec s=A^T\vec b$$                    
                    另一方面，如果我们计算 $A^TA\vec s=A^T\vec b$ ，那么有：
                    
      $$A^TA\vec s=A^T\vec b\Leftrightarrow A^T(\vec b-A\vec s)=0$$                    
                    因此 $\vec b-A\vec s\in \text{Nul}(A^T)=\text{Col}(A)^\perp$，$A\vec s\in\text{Col}(A)$
                    
                    由于正交投影向量唯一确定，所以 $A \vec s=\hat b$
                    
                    因此，$A \vec s=\hat b$ 的解与 $A^TA\vec s=A^T\vec b$ 的解完全相同。
                    
    - 支援向量机时序回归（SVR）
        - 具体操作步骤
        - 数学原理分析
            
            对于回归问题，给定训练用向量集合 $D=\{(x_1,y_1),(x_2,y_2)\cdots(x_n,y_n)\}$
            
            我们希望学习得到一个回归模型 $f_{w,b}(x)=w^Tx+b$，使得 $x_i \approx y_i$。
            
            传统回归模型计算 $\epsilon=f_{w,b}(x_i)-y_i, \ \epsilon \rightarrow 0$，作为损失值，只有输出值 $f_{w,b}(x_i)$ 与真实值 $y_i$ 相同时，损失值才为 $0$。
            
            而在 SVR 模型中，我们规定一个可容忍的误差大小 $\epsilon_{base}$，当 $|f_{w,b}(x_i)-y_i|=\epsilon \leq \epsilon_{base}$ 时，误差处于可容忍的范围内，只有 $\epsilon_{base}<\epsilon$ 时，我们才计算损失。
            
            对此，我们构建正则化函数 $l_\epsilon$：
            
      $$\begin{aligned}    l_\epsilon(x)=   \begin{cases}   0&|x| <\epsilon   \,\\ |x|-\epsilon &|x|\geq \epsilon  \end{cases}  \end{aligned}$$            
            由此，我们得到问题：
            
      $$min_{w,b}\left(\frac{1}{2}\|w\|^2+C\cdot \sum_{i=1}^N l_\epsilon(f(x_i)-y_i)\right)\\f= X^T\cdot\vec{w}+b=0$$            
            对 $l_\epsilon(f(x_i)-y_i)$ 函数，当输入大于可容忍误差时，返回输入与可容忍误差的差，因此我们引入两个松弛变量，将该函数非零输出的值消解：
            
      $$f(x_i)-y_i>\epsilon>0, \ f(x_i)-y_i-\epsilon = \xi_i\\ y_i-f(x_i)>\epsilon>0, \ y_i-f(x_i)-\epsilon = \hat\xi_i$$            
            对输入小于可容忍误差的情况，我们添加约束条件 $\xi_i \geq 0, \hat\xi_i \geq 0$，限制 $\xi_i$ 非负，这样就解决了出现负的松弛变量的情况，解决了 $l_\epsilon$  函数零输出值的部分，直接利用约束条件将其舍弃。
            
            于是我们将原问题重写为约束向量形式：
            
      $$min_{w,b}\left(\frac{1}{2}\|w\|^2+C\cdot \sum_{i=1}^N (\xi_i+\hat\xi_i)\right)\\ \begin{aligned} s.t. \  \ f(x_i)-y_i&\leq\epsilon + \xi_i \\ y_i-f(x_i)&\leq\epsilon + \hat\xi_i \\ \xi _i&\geq 0 \\ \hat\xi_i &\geq 0\end{aligned}$$            
            我们利用不等式约束的拉格朗日乘子法计算最小值：
            
      $$$$