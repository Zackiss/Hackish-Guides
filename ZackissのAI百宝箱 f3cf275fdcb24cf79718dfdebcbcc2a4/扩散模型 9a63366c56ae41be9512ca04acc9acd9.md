# 扩散模型

### 语言图像对照预训练 （CLIP）

### 扩散模型（Diffusion Model）

- 模型架构
    
    <aside>
    💡 扩散模型是由非平衡热力学衍生出来的一种基于概率论的生成模型
    
    </aside>
    
    扩散模型灵感来源于热力学中的扩散现象。将染色剂滴入水中，水中形成高低浓度梯度，扩散现象指分子逐渐从高浓度扩散到低浓度，最终溶质均匀分布的过程。
    
    由此看来，前向扩散过程是噪声加入图像，最终均衡分布的过程，也即溶质溶入水中的过程。
    
    而反向扩散过程则是利用深度学习模型，学习扩散过程，拟合出其反向的汇聚过程。
    
    由原始扩散模型，衍生出了 DDPM 模型，如下对此类模型进行阐释。
    
- 正向扩散过程
    
    <aside>
    💡 正向扩散过程是在清晰图像中逐步添加噪声，最终得到噪声图的过程
    
    </aside>
    
    在前向扩散中，我们逐步给图像加入高斯噪声，随着噪声加入，图像逐渐损失信息。最终，图像完全丧失原有属性，收敛于噪声的正态分布。
    
    $$
    X_1 \rightarrow \cdots \rightarrow X_{t-1}\rightarrow q(X_t\mid X_{t-1})\rightarrow X_t\rightarrow \cdots \rightarrow X_T
    $$
    
    其中，$X_t$ 为由 $X_{t-1}$ 增加噪声之后的矩阵，$q(x)$ 为用于增加噪声的函数，定义如下：
    
    $$
    q(X_t\mid X_{t-1})=\phi (X_t;\ \mu=\sqrt{1-\beta_t}\cdot X_{t-1},\sigma^2=\beta_t)
    $$
    
    其中，$\beta_t$ 为超参数，由于前期噪声对图片影响大，后期对图片影响小，因此其值一般随时间递增，在 DDPM 的设计中，$\beta_1=10^{-4},\ \beta_T=0.02,\ \{\beta_t\in (0, 1)\}^T_{t=1}$
    
    我们观察 $X_t$ 一个元素的值 $x_t$ 的来源，首先对 $x_{t-1}$ 缩放 $\sqrt{1-\beta_t}$ ，接着以 $\sqrt{\beta_{t}}$ 作为标准差，构造出一个以缩放后的 $x_{t-1}$ 为中心的正态分布，从分布中取样本作为 $x_t$ 加噪后的值。
    
    因此 $X_{t}\sim \mathcal N(\sqrt{1-\beta_t}\cdot X_{t-1},\beta_t)$
    
    具体到像素点 $x_t$ 而言，添加噪声本质上是从一个缩放后的标准正态分布取值的过程，我们想要将缩放过程放回到随机变量 $x_t$ 中，将分布回归为标准正态分布，这个过程称为重参数化：
    
    $$
    u\sim \mathcal N(\mu,\sigma^2)\rightarrow u=\mu+\sigma\cdot\epsilon,\ \epsilon\sim \mathcal N(0,1)
    $$
    
    于是我们将 $x_t$ 分布的随机性引到 $z_t$ 上：
    
    $$
    x_{t} = \sqrt{1-\beta_{t}}x_{t-1} + \sqrt{\beta_t}z_{t} \\ z_{t}=\phi (z;\ \mu=0,\sigma^2=1)
    $$
    
    将 $x_{t-2}$ 到 $x_{t-1}$ 的过程代回原式中，化简得到马尔科夫过程：
    
    $$
    x_{t} = \sqrt{(1-\beta_t)(1-\beta_{t-1})}x_{t-2} + \sqrt{(1-\beta_t)\beta_{t-1}}z_{t-1} + \sqrt{\beta_t}z_{t}
    $$
    
    统一来看，从 $x_{t-1}$ 到 $x_t$ 的过程，从 $x_{t-2}$ 到 $x_{t-1}$ 的过程均添加了一个符合标准正态分布的噪声，而从 $x_{t-2}$ 到 $x_t$ 的过程则是添加了两个分别服从 $\mathcal N(0,\beta_{t-1}\cdot(1-\beta_t))$ 与 $\mathcal N(0,\beta_{t})$ 这两个正态分布的噪声。
    
    我们设 $\alpha_t=1-\beta_t$，那么在 $x_{t-2}$ 到 $x_t$ 的过程中，实际添加了一个服从两分布累加的噪声：
    
    $$
    \mathcal N(0,\alpha_{t}\cdot(1-\alpha_{t-1}))+\mathcal N(0,1-\alpha_{t})=\mathcal N(0,1-\alpha_{t-1}\cdot\alpha_{t})
    $$
    
    因此，我们可以直接计算 $x_{t-2}\rightarrow x_{t}$ 的噪声结果：
    
    $$
    x_{t} = \sqrt{\alpha_{t}\alpha_{t-1}}x_{t-2} + \sqrt{1-\alpha_{t}\alpha_{t-1}}\bar{z} \\\bar{z}=\phi (z;\ \mu=0,\sigma^2=1)
    $$
    
    因为整体是一个递推过程，我们从 初始图像可以得到任一阶段加噪图像，归纳出如下式：
    
    $$
    x_{t} = \sqrt{\prod_{m=1}^t{\alpha_{m}}}\cdot x_{0} + \sqrt{1-\prod_{m=1}^t{\alpha_{m}}}{z_{t}}\\ z_t=\phi (z;\ \mu=0,\sigma^2=1)
    $$
    
    因此 $X_{t}\sim \mathcal N\left(\sqrt{\prod_{m=1}^t{\alpha_{m}}}\cdot X_{0},\sqrt{1-\prod_{m=1}^t{\alpha_{m}}}\right)$ ，累计一路下来的噪声添加过程。
    
    并且随着 $\beta_t$ 逐步增大，$\alpha_t$ 逐步减小，$\lim_{t\rightarrow +\infty} \mathcal N\left(\sqrt{\prod_{m=1}^t{\alpha_{t}}}\cdot X_{0},\sqrt{1-\prod_{m=1}^t{\alpha_{t}}}\right)=N(0,1)$，即越往后添加噪声，其结果越服从标准正态分布。
    
- 逆向扩散过程
    
    <aside>
    💡 逆向扩散过程是从随机白噪声出发，反推出某个清晰图像的过程
    
    </aside>
    
    我们从白噪声开始，学习从图像中去除噪声，生成数据。最终生成清晰图像。
    
    总的来讲，我们学习得到一个参数化的逆向高斯分布 $p_{\theta}$，其对应正向高斯分布为 $p(x_t\mid x_{t-1})$
    
    我们有贝叶斯公式：
    
    $$
    q(x_{t-1}\mid x_{t}) = q(x_{t}\mid x_{t-1}) \frac{q(x_{t-1}) }{q(x_{t})}
    $$
    
    因为前向传播过程中，$x_0$ 一直是确定的，因此有：
    
    $$
    q(x_{t-1}\mid x_{t},x_{0}) = q(x_{t}\mid x_{t-1},x_{0}) \frac{q(x_{t-1}\mid x_{0}) }{q(x_{t}\mid x_{0})}
    $$
    
    - 对于右侧三项，通过正向扩散过程的重参数化，我们均能得知其分布状况
        1. $q(x_{t}|x_{t-1},x_{0}) = \sqrt{\alpha_{t}}x_{t-1} + \sqrt{1-\alpha_{t}}z$
        2. $q(x_{t}|x_{0}) = \sqrt{\prod_{m=1}^t{\alpha_{m}}}\cdot x_{0} + \sqrt{1-\prod_{m=1}^t\alpha_{m}}\cdot{z}$ 
        3. $q(x_{t-1}|x_{0}) = \sqrt{\prod_{m=1}^{t-1}{\alpha_{m}}}\cdot x_{0} + \sqrt{1-\prod_{m=1}^{t-1}\alpha_{m}}\cdot{z}$ 
    - 因此这三项服从三个正态分布
        
        $q(x_{t}|x_{0})\sim N\left( \sqrt{\prod_{m=1}^t{\alpha_{m}}}\cdot x_{0},\sqrt{1-\prod_{m=1}^t\alpha_{m}}\right)$
        
        $q(x_{t}|x_{t-1},x_{0})\sim N( \sqrt{\alpha_{t}}x_{t-1},1-\alpha_{t})$
        
        $q(x_{t-1}|x_{0})\sim N\left( \sqrt{\prod_{m=1}^{t-1}{\alpha_{m}}}\cdot x_{0},\sqrt{1-\prod_{m=1}^{t-1}\alpha_{m}}\right)$
        
    
    代入原式，我们能通过如下步骤推导出 $q(x_{t-1}\mid x_{t},x_{0})$ 服从的正态分布
    
    我们将 $\prod_{m=1}^t{\alpha_{m}}$ 简写为 $\bar{\alpha}_m$，于是有：
    
    $$
    q(x_{t-1}|x_{t},x_{0})\propto e^{-\frac{1}{2}(\frac{(x_{t}-\sqrt{\alpha_{t}}x_{t-1})^{2}}{β_{t}}+\frac{(x_{t-1}-\sqrt{\bar\alpha_{t-1}}x_{0})^{2}}{1-\bar\alpha_{t-1}}-\frac{(x_{t}-\sqrt{\bar\alpha_{t}}x_{0})^{2}}{1-\bar\alpha_{t}})}
    $$
    
    化简得到如下指数：$-\frac{1}{2}[{(\frac{\alpha_{t}}{β_{t}}+\frac{1}{1-\bar\alpha_{t-1}})}{x_{t-1}^{2}}-{(\frac{2\sqrt{\alpha_{t}}}{β_{t}}x_{t}+\frac{2\sqrt{\bar\alpha_{t-1}}}{1-\bar\alpha_{t-1}}x_{0})x_{t-1}}+C]$
    
    与一般正态分布指数对应：$-\frac{1}{2}(\frac{1}{\sigma^{2}}x^{2}-\frac{2\mu}{\sigma^{2}}x+\frac{\mu^{2}}{\sigma^{2}})$
    
    因此，我们有 $\frac{2\mu}{\sigma^{2}}=(\frac{2\sqrt{\alpha_{t}}}{β_{t}}x_{t}+\frac{2\sqrt{\bar\alpha_{t-1}}}{1-\bar\alpha_{t-1}}x_{0})$，$\frac{1}{\sigma^{2}}=\frac{\alpha_{t}}{\beta_{t}}+\frac{1}{1-\bar\alpha_{t-1}}$
    
    于是得到 $\mu=(\frac{\sqrt{\alpha_{t}}{(1-\bar\alpha_{t-1})}}{{1-\bar\alpha_{t}}}x_{t}+\frac{\sqrt{\bar\alpha_{t-1}}{\beta_{t}}}{1-\bar\alpha_{t}}x_{0})$
    
    利用 $x_{t} = \sqrt{\bar{\alpha}_{t}}x_{0} + \sqrt{1-\bar{\alpha}_{t}}{z_{t}}$，将 $\mu$ 中的 $x_0$ 消去，得到 $x_t$，$\beta_t$ 与 $\mu$ 的关系。
    
    最终，我们得到 $q(x_{t-1}|x_{t},x_{0})$ 服从正态分布的均值与方差：
    
    $$
    \frac{1}{\sigma^{2}}=\frac{\alpha_{t}}{\beta_{t}}+\frac{1}{1-\bar\alpha_{t-1}}
    $$
    
    $$
    \mu=\frac{1}{\sqrt{\alpha_{t}}}(x_{t}-\frac{1-\alpha_{t}}{\sqrt{1-\bar\alpha_{t}}}z_{t})
    $$
    
    我们的目的是通过训练得到 $q(x_{t-1}|x_{t},x_{0})$ 服从的正态分布，然后利用这个正态分布，从 $x_T$ 逆推回 $x_0$，这也是逆向扩散的本质。
    
- 逆扩散过程学习
    
    首先进行正向扩散，然后将得到的数据放入神经网络。
    
    对每一个时间点 $t$，我们都有数据 $t$， $x_t$ 与 $z_t$，其中 $z_t$ 为正向扩散时标准正态分布的采样结果，通过训练得到输入一个 $(t,x_t)$ 输出 $z_t$，即从一已加噪声后的 $x_t$ 图像中反演出原噪声 $z_t$。
    
    那么就有了整体的噪声复原过程：
    
    1. 从 $x_T$ 白噪声图出发，利用 $x_t$ 与 $t$ 预测噪声 $z_{t}$
    2. 利用 $(t,x_t,z_t)$，目的图像（艺术品）的像素点也符合一个正态分布 $q(x_{t-1}|x_{t},x_{0})$
    3. 对这个正态分布采样，得到 $x_{t}$ 对应的原图像素点 $x_{t-1}$，对 $X_t$ 矩阵整体操作一遍。
    4. 最终得到 $X_0$
    
    对于训练逆扩散过程的神经网络而言，这是一个监督学习过程。我们需要用到交叉熵与 KL 散度，以真实分布与预测分布的交叉熵为损失函数：
    
    以真实分布 $q(x)$ 为标杆，计算 $q(x)$ 与预测分布 $\hat q(x)$ 的交叉熵，一般而言， $P$ 相对于 $Q$ 的交叉熵写作 $\mathbb E_Q(-\log P)$：
    
    $$
    \mathbb E_{q(x_0)}[-\log(\hat q(x_0))]=\mathbb E_{q(x_0)}[-\log(\int \hat q(x_{0:r})\ dx_{1:r})]
    $$
    
    另一方面，从最大似然的角度来看，我们希望含有各种参数（这里简写为 $\theta$）的预测分布 $\log \hat q_\theta(x_0)$ 越大越好。究其原因，$x_0$ 是模型的目标图的像素点，也是似然中的事实，我们需要一个预测函数 $\hat q_\theta(x)$，调整参数 $\theta$，使得代入像素点 $x_0$ 时，这个预测函数的概率值最大。如果这样，当我们利用这个预测函数去生成图片时，生成图片才与真实图片最接近，所以有：
    
    $$
    \argmax_\theta(\log \hat q_\theta(x_0))
    $$
    
    在这里，我们利用交叉熵进行下一阶段的推导：
    
    $$
    E_c=\mathbb E_{q(x_0)}[-\log(\hat q(x_0))]=\mathbb E_{q(x_0)}[-\log(\int \hat q(x_{0:T})\ dx_{1:T})]
    $$
    
    上式中，$q(x_{0:T})=\prod_{t=1}^Tq(x_t)$ ，$dx_{0:T}=x_0\cdot dx_{1:T}$，$x_0$ 为常数，不影响极大值取值。
    
    基于 $p$ 事实，对 $q$ 的期望表示为 $\mathbb E_p(q)=\int p\cdot q \ dr$，因此，我们可以凑出期望：
    
    $$
    \begin{aligned}E_c=&\mathbb E_{q(x_0)}[-\log(\int \hat q(x_{0:T})\ dx_{1:T})]\\=&\mathbb E_{q(x_0)}[-\log(\int q(x_{1:T}|x_0)\cdot\frac{\hat q(x_{0:T})}{q(x_{1:T}|x_0)}\ dx_{1:T})] \\=&\mathbb E_{q(x_0)}[-\log[\mathbb E_{q(x_{1:T}|x_0)}(\frac{\hat q(x_{0:T})}{q(x_{1:T}|x_0)})]]\end{aligned}
    $$
    
    对于凸函数，我们有詹森（Jensen）不等式：
    
    $$
    f(\sum_{i=1}^n\frac{x_i}{n})\geq \frac{\sum_{i=1}^n f(x_i)}{n}
    $$
    
    因此，$f(E(x))\geq E(f(x))$，借助此不等式，我们构造出交叉熵的上界：
    
    $$
    L_{vb}=\mathbb E_{q(x_{1:T}|x_0)}\left[\log\left(\frac{q(x_{1:T}|x_0)}{\hat q(x_{0:T})}\right)\right]\geq E_c
    $$
    
    我们设 $p_\theta=\hat q$，利用 $q(x_{0:T})=\prod_{t=1}^Tq(x_t)$ 展开此上界 $L_{vb}$：
    
    $$
    \begin{aligned} L_{vb}  &= \mathbb{E}_{q(\mathbf{x}_{0:T})} \Big[ \log\frac{\prod_{t=1}^T q(\mathbf{x}_t\vert\mathbf{x}_{t-1})}{ p_\theta(\mathbf{x}_T) \prod_{t=1}^T p_\theta(\mathbf{x}_{t-1} \vert\mathbf{x}_t) } \Big] \\ &= \mathbb{E}_{q(\mathbf{x}_{0:T})} \Big[ -\log p_\theta(\mathbf{x}_T) + \sum_{t=1}^T \log \frac{q(\mathbf{x}_t\vert\mathbf{x}_{t-1})}{p_\theta(\mathbf{x}_{t-1} \vert\mathbf{x}_t)}\Big] \\ &= \mathbb{E}_{q(\mathbf{x}_{0:T})} \Big[ -\log p_\theta(\mathbf{x}_T) + \sum_{t=2}^T \log \frac{q(\mathbf{x}_t\vert\mathbf{x}_{t-1})}{p_\theta(\mathbf{x}_{t-1} \vert\mathbf{x}_t)} + \log\frac{q(\mathbf{x}_1 \vert \mathbf{x}_0)}{p_\theta(\mathbf{x}_0 \vert \mathbf{x}_1)} \Big]\end{aligned}
    $$
    
    我们利用贝叶斯公式 $q(x_{t-1}\mid x_{t}) = q(x_{t}\mid x_{t-1}) \frac{q(x_{t-1}) }{q(x_{t})}$ ，将前向分布变为逆向分布，再利用$\log(\frac{a}{b})+\log(\frac{b}{c})=\log(\frac{a}{b}\cdot\frac{b}{c})=\log(\frac{a}{c})$ 消去累加：
    
    $$
    \\ L_{vb}= \mathbb{E}_{q(\mathbf{x}_{0:T})} \Big[ -\log p_\theta(\mathbf{x}_T) + L_{mid} + \log \frac{q(\mathbf{x}_1 \vert \mathbf{x}_0)}{p_\theta(\mathbf{x}_0 \vert \mathbf{x}_1)} \Big] \\\begin{aligned}L_{mid}&=\sum_{t=2}^T \log \Big( \frac{{q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0)}}{p_\theta(\mathbf{x}_{t-1} \vert\mathbf{x}_t)}\cdot \frac{{q(\mathbf{x}_t \vert \mathbf{x}_0)}}{{q(\mathbf{x}_{t-1}\vert\mathbf{x}_0)}} \Big)\\&= \sum_{t=2}^T \log \frac{q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0)}{p_\theta(\mathbf{x}_{t-1} \vert\mathbf{x}_t)} + \sum_{t=2}^T \log \frac{q(\mathbf{x}_t \vert \mathbf{x}_0)}{q(\mathbf{x}_{t-1} \vert \mathbf{x}_0)} \\ &=\sum_{t=2}^T \log \frac{q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0)}{p_\theta(\mathbf{x}_{t-1} \vert\mathbf{x}_t)} + \log\frac{q(\mathbf{x}_T \vert \mathbf{x}_0)}{q(\mathbf{x}_1 \vert \mathbf{x}_0)}\end{aligned}
    $$
    
    利用 $\log\frac{q(\mathbf{x}_T \vert \mathbf{x}_0)}{q(\mathbf{x}_1 \vert \mathbf{x}_0)}+\log \frac{q(\mathbf{x}_1 \vert \mathbf{x}_0)}{p_\theta(\mathbf{x}_0 \vert \mathbf{x}_1)}=- \log p_\theta(\mathbf{x}_0 \vert \mathbf{x}_1)$， 进一步消去 $L_{vb}$： 
    
    $$
    L_{vb}= \mathbb{E}_{q(\mathbf{x}_{0:T})} \Big[ -\log p_\theta(\mathbf{x}_T) + \sum_{t=2}^T \log \frac{q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0)}{p_\theta(\mathbf{x}_{t-1} \vert\mathbf{x}_t)}- \log p_\theta(\mathbf{x}_0 \vert \mathbf{x}_1) \Big]
    $$
    
    最终我们得到影响交叉熵的一项为 $q(x_{t-1}\mid x_t,x_0)$ 与 $\hat q(x_{t-1}\mid x_t)$ 的 KL 散度：
    
    $$
    L_t=\mathbb E_{q(x_0:T)}(D_{KL}[q(x_{t-1}\mid x_t,x_0)\Vert \hat q(x_{t-1}\mid x_t)])
    $$
    
    这也正好是真实分布 $q$ 与预测分布 $\hat q$ 之间的 KL 散度，在逆向扩散过程中，我们得知这两个分布均服从各自的正态分布，两个正态分布的 KL 散度如下：
    
    $$
    D_{KL}(\mathcal N(\mu_1, \sigma_1^2\Vert N(\mu_2, \sigma_2^2))=\log \frac{\sigma_2}{\sigma_1}+\frac{\sigma_1^2+(\mu_1-\mu_2)^2}{2\cdot \sigma_2^2}-\frac{1}{2}
    $$
    
    对于目前情况，两个正态分布均有均值 $\mu=\frac{1}{\sqrt{\alpha_{t}}}(x_{t}-\frac{1-\alpha_{t}}{\sqrt{1-\bar\alpha_{t}}}z_{t})$ ，方差 $\frac{1}{\sigma^{2}}=\frac{\alpha_{t}}{\beta_{t}}+\frac{1}{1-\bar\alpha_{t-1}}$，不同之处在于真实分布的 $x_t$ 和 $\beta_t$ 是正向扩散得到的数据，而预测分布中这些是作为待训练参数，表示为 $\theta$ 出现的（如果求最大似然，要对这些参数求偏导数）。
    
    因此我们代入计算，最终得到 $L_t$ ：